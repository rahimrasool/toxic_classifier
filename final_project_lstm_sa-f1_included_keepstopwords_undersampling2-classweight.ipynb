{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\K1NG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\K1NG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\K1NG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\K1NG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import combinations\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import texthero as hero\n",
    "import regex as re\n",
    "\n",
    "\n",
    "from torchtext.vocab import FastText\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "from collections import Counter\n",
    "from torchtext.vocab import vocab\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download('punkt')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove those rows where tokens are empty\n",
    "# Do not remove stop words or lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>identity</th>\n",
       "      <th>train_set</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>240311</td>\n",
       "      <td>You're an idiot.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>idiot</td>\n",
       "      <td>[idiot]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>241282</td>\n",
       "      <td>NIGGER</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>nigger</td>\n",
       "      <td>[nigger]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3669</th>\n",
       "      <td>245669</td>\n",
       "      <td>SNITCH</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>snitch</td>\n",
       "      <td>[snitch]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10992</th>\n",
       "      <td>255750</td>\n",
       "      <td>your an idiot</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>idiot</td>\n",
       "      <td>[idiot]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13365</th>\n",
       "      <td>258686</td>\n",
       "      <td>cut them.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>cut</td>\n",
       "      <td>[cut]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1896557</th>\n",
       "      <td>7091683</td>\n",
       "      <td>You are the cretin.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>cretin</td>\n",
       "      <td>[cretin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1896762</th>\n",
       "      <td>7091888</td>\n",
       "      <td>Hypocrite!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>hypocrite</td>\n",
       "      <td>[hypocrite]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1898580</th>\n",
       "      <td>7093706</td>\n",
       "      <td>Loser.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>loser</td>\n",
       "      <td>[loser]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900680</th>\n",
       "      <td>7095806</td>\n",
       "      <td>Idiot.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>idiot</td>\n",
       "      <td>[idiot]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1901041</th>\n",
       "      <td>7096167</td>\n",
       "      <td>Ridiculous.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>ridiculous</td>\n",
       "      <td>[ridiculous]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1462 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id         comment_text  identity  train_set  label  \\\n",
       "289      240311   You're an idiot.     0         1          1       \n",
       "956      241282   NIGGER               1         1          1       \n",
       "3669     245669   SNITCH               0         1          1       \n",
       "10992    255750   your an idiot        0         1          1       \n",
       "13365    258686   cut them.            0         1          1       \n",
       "...         ...         ...           ..        ..         ..       \n",
       "1896557  7091683  You are the cretin.  0         0          1       \n",
       "1896762  7091888  Hypocrite!           0         0          1       \n",
       "1898580  7093706  Loser.               0         0          1       \n",
       "1900680  7095806  Idiot.               0         0          1       \n",
       "1901041  7096167  Ridiculous.          0         0          1       \n",
       "\n",
       "          clean_text        tokens  \n",
       "289         idiot     [idiot]       \n",
       "956      nigger       [nigger]      \n",
       "3669     snitch       [snitch]      \n",
       "10992      idiot      [idiot]       \n",
       "13365    cut          [cut]         \n",
       "...        ...          ...         \n",
       "1896557     cretin    [cretin]      \n",
       "1896762  hypocrite    [hypocrite]   \n",
       "1898580  loser        [loser]       \n",
       "1900680  idiot        [idiot]       \n",
       "1901041  ridiculous   [ridiculous]  \n",
       "\n",
       "[1462 rows x 7 columns]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.2+cu113 0.11.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check your version\n",
    "# On Colab, you'll see ('1.10.2+cu102', '0.11.2')\n",
    "print(torch.__version__, torchtext.__version__)\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n",
    "test = pd.read_csv('./data/jigsaw-unintended-bias-in-toxicity-classification/test_public_expanded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    1804874\n",
      "Name: target, dtype: int64\n",
      "False    97320\n",
      "Name: toxicity, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train[\"target\"].isna().value_counts())\n",
    "train[\"label\"] = train[\"target\"].apply(lambda x: 1 if x>=0.5 else 0)\n",
    "print(test[\"toxicity\"].isna().value_counts())\n",
    "test[\"label\"] = test[\"toxicity\"].apply(lambda x: 1 if x>=0.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\K1NG\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\K1NG\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "IDENTITY_COLUMNS = ['asian', 'atheist', 'bisexual',\n",
    "       'black', 'buddhist', 'christian', 'female', 'heterosexual', 'hindu',\n",
    "       'homosexual_gay_or_lesbian', 'intellectual_or_learning_disability',\n",
    "       'jewish', 'latino', 'male', 'muslim', 'other_disability',\n",
    "       'other_gender', 'other_race_or_ethnicity', 'other_religion',\n",
    "       'other_sexual_orientation', 'physical_disability',\n",
    "       'psychiatric_or_mental_illness', 'transgender', 'white']\n",
    "\n",
    "train[\"identity\"] = (train[IDENTITY_COLUMNS].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int)\n",
    "test[\"identity\"] = (test[IDENTITY_COLUMNS].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1634570\n",
       "1     170304\n",
       "Name: identity, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"identity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"train_set\"] = 1\n",
    "test[\"train_set\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let us select relevant columns of train and test set and merge them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>identity</th>\n",
       "      <th>train_set</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59852</td>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59855</td>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59856</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902189</th>\n",
       "      <td>7097315</td>\n",
       "      <td>That's the thing...it's called a plan.  Get in...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902190</th>\n",
       "      <td>7097316</td>\n",
       "      <td>It's not quite the way you describe it, Mike. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902191</th>\n",
       "      <td>7097317</td>\n",
       "      <td>What right have you to criticise? You have no ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902192</th>\n",
       "      <td>7097318</td>\n",
       "      <td>My concern is that if China allies with the US...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902193</th>\n",
       "      <td>7097319</td>\n",
       "      <td>Dear Young Philip, General Kelly can speak for...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1902194 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                       comment_text  identity  \\\n",
       "0          59848  This is so cool. It's like, 'would you want yo...         0   \n",
       "1          59849  Thank you!! This would make my life a lot less...         0   \n",
       "2          59852  This is such an urgent design problem; kudos t...         0   \n",
       "3          59855  Is this something I'll be able to install on m...         0   \n",
       "4          59856               haha you guys are a bunch of losers.         0   \n",
       "...          ...                                                ...       ...   \n",
       "1902189  7097315  That's the thing...it's called a plan.  Get in...         0   \n",
       "1902190  7097316  It's not quite the way you describe it, Mike. ...         0   \n",
       "1902191  7097317  What right have you to criticise? You have no ...         0   \n",
       "1902192  7097318  My concern is that if China allies with the US...         0   \n",
       "1902193  7097319  Dear Young Philip, General Kelly can speak for...         0   \n",
       "\n",
       "         train_set  label  \n",
       "0                1      0  \n",
       "1                1      0  \n",
       "2                1      0  \n",
       "3                1      0  \n",
       "4                1      1  \n",
       "...            ...    ...  \n",
       "1902189          0      0  \n",
       "1902190          0      0  \n",
       "1902191          0      0  \n",
       "1902192          0      0  \n",
       "1902193          0      0  \n",
       "\n",
       "[1902194 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = train[[\"id\", \"comment_text\", \"identity\", \"train_set\", \"label\"]]\n",
    "df2 = test[[\"id\", \"comment_text\", \"identity\", \"train_set\", \"label\"]]\n",
    "\n",
    "data = df1.append(df2, ignore_index=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "\n",
    "#### 1. Creating target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Distribution of identities'}>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1008x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data[\"identity\"].value_counts().plot(kind=\"bar\", figsize=(14,8), title=\"Distribution of identities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Distribution of class labels'}>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1008x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data[\"label\"].value_counts().plot(kind=\"bar\", figsize=(14,8), title=\"Distribution of class labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\K1NG\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "# data[[\"comment_text\", \"label\"]].head(100)\n",
    "# sh*tty\n",
    "# ur\n",
    "# hahahahahahahahhha\n",
    "# \\nsheriff\n",
    "# whaaaaaatttttt?\n",
    "# waaaaay\n",
    "# BwaaaaaaaaaHahahahahaha\n",
    "# aaaaalmost\n",
    "# bbbaaaaaa\n",
    "# Whaaaaaat\n",
    "# DISCUSSTING\n",
    "\n",
    "# remove punctuation and replace with an empty single space\n",
    "# lower level\n",
    "# downsampling is also important \n",
    "# data[data[\"comment_text\"].str.contains(\"aaaaa\") == True][[\"comment_text\", \"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>identity</th>\n",
       "      <th>train_set</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>this is so cool  it s like   would you want yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>thank you  this would make my life a lot less ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59852</td>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>this is such an urgent design problem  kudos t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59855</td>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>is this something i ll be able to install on m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59856</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>haha you guys are a bunch of losers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902189</th>\n",
       "      <td>7097315</td>\n",
       "      <td>That's the thing...it's called a plan.  Get in...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>that s the thing it s called a plan   get into...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902190</th>\n",
       "      <td>7097316</td>\n",
       "      <td>It's not quite the way you describe it, Mike. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>it s not quite the way you describe it  mike  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902191</th>\n",
       "      <td>7097317</td>\n",
       "      <td>What right have you to criticise? You have no ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>what right have you to criticise  you have no ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902192</th>\n",
       "      <td>7097318</td>\n",
       "      <td>My concern is that if China allies with the US...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>my concern is that if china allies with the us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902193</th>\n",
       "      <td>7097319</td>\n",
       "      <td>Dear Young Philip, General Kelly can speak for...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dear young philip  general kelly can speak for...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1902194 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                       comment_text  identity  \\\n",
       "0          59848  This is so cool. It's like, 'would you want yo...         0   \n",
       "1          59849  Thank you!! This would make my life a lot less...         0   \n",
       "2          59852  This is such an urgent design problem; kudos t...         0   \n",
       "3          59855  Is this something I'll be able to install on m...         0   \n",
       "4          59856               haha you guys are a bunch of losers.         0   \n",
       "...          ...                                                ...       ...   \n",
       "1902189  7097315  That's the thing...it's called a plan.  Get in...         0   \n",
       "1902190  7097316  It's not quite the way you describe it, Mike. ...         0   \n",
       "1902191  7097317  What right have you to criticise? You have no ...         0   \n",
       "1902192  7097318  My concern is that if China allies with the US...         0   \n",
       "1902193  7097319  Dear Young Philip, General Kelly can speak for...         0   \n",
       "\n",
       "         train_set  label                                         clean_text  \n",
       "0                1      0  this is so cool  it s like   would you want yo...  \n",
       "1                1      0  thank you  this would make my life a lot less ...  \n",
       "2                1      0  this is such an urgent design problem  kudos t...  \n",
       "3                1      0  is this something i ll be able to install on m...  \n",
       "4                1      1               haha you guys are a bunch of losers   \n",
       "...            ...    ...                                                ...  \n",
       "1902189          0      0  that s the thing it s called a plan   get into...  \n",
       "1902190          0      0  it s not quite the way you describe it  mike  ...  \n",
       "1902191          0      0  what right have you to criticise  you have no ...  \n",
       "1902192          0      0  my concern is that if china allies with the us...  \n",
       "1902193          0      0  dear young philip  general kelly can speak for...  \n",
       "\n",
       "[1902194 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(data):\n",
    "    '''\n",
    "    \"Try with stop words and without stop words\n",
    "    Try with and without lemmatization\n",
    "    '''\n",
    "    # punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "    # for p in punct:\n",
    "    #     data[\"text\"] = data.replace(p, ' ')\n",
    "\n",
    "\n",
    "    # data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n",
    "    # return data\n",
    "    # data[\"clean_text\"] = data[\"comment_text\"].apply(lambda x: x.replace(r'\\n',  ' '))\n",
    "    data[\"clean_text\"] = data[\"comment_text\"].apply(lambda x: re.sub(r'\\n', \" \", x))\n",
    "    data[\"clean_text\"] = data[\"clean_text\"].replace(r'\\\\n',' ', regex=True)\n",
    "    data[\"clean_text\"] = data[\"clean_text\"].replace('\\n',' ', regex=True)\n",
    "    # data[\"clean_text\"] = data[\"comment_text\"].apply(lambda x: x.replace('\\n',  ' '))\n",
    "\n",
    "    custom_pipeline = [\n",
    "        # hero.preprocessing.remove_whitespace,\n",
    "        # hero.preprocessing.remove_digits,\n",
    "        hero.preprocessing.remove_punctuation,\n",
    "        hero.preprocessing.lowercase\n",
    "        # hero.preprocessing.remove_stopwords\n",
    "        ]\n",
    "\n",
    "    data['clean_text'] = hero.clean(data['comment_text'], custom_pipeline)\n",
    "    # data[\"clean_text\"] = data[\"clean_text\"].apply(lambda x: re.sub(r'[0-9]', \"\", x))\n",
    "    # punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "    data[\"clean_text\"] = data[\"clean_text\"].apply(lambda x: re.sub(r\"/-'?!.,#$%\\'()+-/:;<=>@[\\\\]^_`{|}~`\", \"\", x))\n",
    "    data[\"clean_text\"] = data[\"clean_text\"].apply(lambda x: re.sub(r'\"\"“”’', \"\", x))\n",
    "    data[\"clean_text\"] = data[\"clean_text\"].apply(lambda x: re.sub(r'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&', \"\", x))\n",
    "\n",
    "    return data\n",
    "\n",
    "preprocess(data)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.sort_values(by=\"label\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"tokens\"] = data[\"clean_text\"].apply(lambda x: nltk.tokenize.word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>identity</th>\n",
       "      <th>train_set</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>this is so cool  it s like   would you want yo...</td>\n",
       "      <td>[this, is, so, cool, it, s, like, would, you, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>thank you  this would make my life a lot less ...</td>\n",
       "      <td>[thank, you, this, would, make, my, life, a, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59852</td>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>this is such an urgent design problem  kudos t...</td>\n",
       "      <td>[this, is, such, an, urgent, design, problem, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59855</td>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>is this something i ll be able to install on m...</td>\n",
       "      <td>[is, this, something, i, ll, be, able, to, ins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59856</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>haha you guys are a bunch of losers</td>\n",
       "      <td>[haha, you, guy, are, a, bunch, of, loser]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902189</th>\n",
       "      <td>7097315</td>\n",
       "      <td>That's the thing...it's called a plan.  Get in...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>that s the thing it s called a plan   get into...</td>\n",
       "      <td>[that, s, the, thing, it, s, called, a, plan, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902190</th>\n",
       "      <td>7097316</td>\n",
       "      <td>It's not quite the way you describe it, Mike. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>it s not quite the way you describe it  mike  ...</td>\n",
       "      <td>[it, s, not, quite, the, way, you, describe, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902191</th>\n",
       "      <td>7097317</td>\n",
       "      <td>What right have you to criticise? You have no ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>what right have you to criticise  you have no ...</td>\n",
       "      <td>[what, right, have, you, to, criticise, you, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902192</th>\n",
       "      <td>7097318</td>\n",
       "      <td>My concern is that if China allies with the US...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>my concern is that if china allies with the us...</td>\n",
       "      <td>[my, concern, is, that, if, china, ally, with,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902193</th>\n",
       "      <td>7097319</td>\n",
       "      <td>Dear Young Philip, General Kelly can speak for...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dear young philip  general kelly can speak for...</td>\n",
       "      <td>[dear, young, philip, general, kelly, can, spe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1902194 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                       comment_text  identity  \\\n",
       "0          59848  This is so cool. It's like, 'would you want yo...         0   \n",
       "1          59849  Thank you!! This would make my life a lot less...         0   \n",
       "2          59852  This is such an urgent design problem; kudos t...         0   \n",
       "3          59855  Is this something I'll be able to install on m...         0   \n",
       "4          59856               haha you guys are a bunch of losers.         0   \n",
       "...          ...                                                ...       ...   \n",
       "1902189  7097315  That's the thing...it's called a plan.  Get in...         0   \n",
       "1902190  7097316  It's not quite the way you describe it, Mike. ...         0   \n",
       "1902191  7097317  What right have you to criticise? You have no ...         0   \n",
       "1902192  7097318  My concern is that if China allies with the US...         0   \n",
       "1902193  7097319  Dear Young Philip, General Kelly can speak for...         0   \n",
       "\n",
       "         train_set  label                                         clean_text  \\\n",
       "0                1      0  this is so cool  it s like   would you want yo...   \n",
       "1                1      0  thank you  this would make my life a lot less ...   \n",
       "2                1      0  this is such an urgent design problem  kudos t...   \n",
       "3                1      0  is this something i ll be able to install on m...   \n",
       "4                1      1               haha you guys are a bunch of losers    \n",
       "...            ...    ...                                                ...   \n",
       "1902189          0      0  that s the thing it s called a plan   get into...   \n",
       "1902190          0      0  it s not quite the way you describe it  mike  ...   \n",
       "1902191          0      0  what right have you to criticise  you have no ...   \n",
       "1902192          0      0  my concern is that if china allies with the us...   \n",
       "1902193          0      0  dear young philip  general kelly can speak for...   \n",
       "\n",
       "                                                    tokens  \n",
       "0        [this, is, so, cool, it, s, like, would, you, ...  \n",
       "1        [thank, you, this, would, make, my, life, a, l...  \n",
       "2        [this, is, such, an, urgent, design, problem, ...  \n",
       "3        [is, this, something, i, ll, be, able, to, ins...  \n",
       "4               [haha, you, guy, are, a, bunch, of, loser]  \n",
       "...                                                    ...  \n",
       "1902189  [that, s, the, thing, it, s, called, a, plan, ...  \n",
       "1902190  [it, s, not, quite, the, way, you, describe, i...  \n",
       "1902191  [what, right, have, you, to, criticise, you, h...  \n",
       "1902192  [my, concern, is, that, if, china, ally, with,...  \n",
       "1902193  [dear, young, philip, general, kelly, can, spe...  \n",
       "\n",
       "[1902194 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatize_text(text):\n",
    "    '''\n",
    "    '''\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(w) for w in text]\n",
    "\n",
    "data[\"tokens\"] = data[\"tokens\"].apply(lemmatize_text)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#stopwords = stopwords.words('english')\n",
    "# stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Remove single-character tokens (mostly punctuation)\n",
    "# data[\"tokens\"] = data[\"tokens\"].apply(lambda x: [word for word in x if len(word) > 1])\n",
    "\n",
    "# # Remove numbers\n",
    "# #words = [word for word in words if not word.isnumeric()]\n",
    "\n",
    "# # Remove punctuation\n",
    "# # words = [word for word in words if word.isalpha()]\n",
    "\n",
    "# # Lowercase all words (default_stopwords are lowercase too)\n",
    "# words = [word.lower() for word in words]\n",
    "\n",
    "# # Remove stopwords\n",
    "# words = [word for word in words if word not in stopwords]\n",
    "\n",
    "# fdist = nltk.FreqDist(words)\n",
    "\n",
    "# print(fdist)\n",
    "\n",
    "# #fdist.items() - will give all words\n",
    "# fdist.most_common(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>identity</th>\n",
       "      <th>train_set</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sample_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>this is so cool  it s like   would you want yo...</td>\n",
       "      <td>[this, is, so, cool, it, s, like, would, you, ...</td>\n",
       "      <td>0.808275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>thank you  this would make my life a lot less ...</td>\n",
       "      <td>[thank, you, this, would, make, my, life, a, l...</td>\n",
       "      <td>0.808275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59852</td>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>this is such an urgent design problem  kudos t...</td>\n",
       "      <td>[this, is, such, an, urgent, design, problem, ...</td>\n",
       "      <td>0.808275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59855</td>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>is this something i ll be able to install on m...</td>\n",
       "      <td>[is, this, something, i, ll, be, able, to, ins...</td>\n",
       "      <td>0.808275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59856</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>haha you guys are a bunch of losers</td>\n",
       "      <td>[haha, you, guy, are, a, bunch, of, loser]</td>\n",
       "      <td>1.616550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902189</th>\n",
       "      <td>7097315</td>\n",
       "      <td>That's the thing...it's called a plan.  Get in...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>that s the thing it s called a plan   get into...</td>\n",
       "      <td>[that, s, the, thing, it, s, called, a, plan, ...</td>\n",
       "      <td>0.808275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902190</th>\n",
       "      <td>7097316</td>\n",
       "      <td>It's not quite the way you describe it, Mike. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>it s not quite the way you describe it  mike  ...</td>\n",
       "      <td>[it, s, not, quite, the, way, you, describe, i...</td>\n",
       "      <td>0.808275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902191</th>\n",
       "      <td>7097317</td>\n",
       "      <td>What right have you to criticise? You have no ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>what right have you to criticise  you have no ...</td>\n",
       "      <td>[what, right, have, you, to, criticise, you, h...</td>\n",
       "      <td>0.808275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902192</th>\n",
       "      <td>7097318</td>\n",
       "      <td>My concern is that if China allies with the US...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>my concern is that if china allies with the us...</td>\n",
       "      <td>[my, concern, is, that, if, china, ally, with,...</td>\n",
       "      <td>0.808275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902193</th>\n",
       "      <td>7097319</td>\n",
       "      <td>Dear Young Philip, General Kelly can speak for...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dear young philip  general kelly can speak for...</td>\n",
       "      <td>[dear, young, philip, general, kelly, can, spe...</td>\n",
       "      <td>0.808275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1902194 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                       comment_text  identity  \\\n",
       "0          59848  This is so cool. It's like, 'would you want yo...         0   \n",
       "1          59849  Thank you!! This would make my life a lot less...         0   \n",
       "2          59852  This is such an urgent design problem; kudos t...         0   \n",
       "3          59855  Is this something I'll be able to install on m...         0   \n",
       "4          59856               haha you guys are a bunch of losers.         0   \n",
       "...          ...                                                ...       ...   \n",
       "1902189  7097315  That's the thing...it's called a plan.  Get in...         0   \n",
       "1902190  7097316  It's not quite the way you describe it, Mike. ...         0   \n",
       "1902191  7097317  What right have you to criticise? You have no ...         0   \n",
       "1902192  7097318  My concern is that if China allies with the US...         0   \n",
       "1902193  7097319  Dear Young Philip, General Kelly can speak for...         0   \n",
       "\n",
       "         train_set  label                                         clean_text  \\\n",
       "0                1      0  this is so cool  it s like   would you want yo...   \n",
       "1                1      0  thank you  this would make my life a lot less ...   \n",
       "2                1      0  this is such an urgent design problem  kudos t...   \n",
       "3                1      0  is this something i ll be able to install on m...   \n",
       "4                1      1               haha you guys are a bunch of losers    \n",
       "...            ...    ...                                                ...   \n",
       "1902189          0      0  that s the thing it s called a plan   get into...   \n",
       "1902190          0      0  it s not quite the way you describe it  mike  ...   \n",
       "1902191          0      0  what right have you to criticise  you have no ...   \n",
       "1902192          0      0  my concern is that if china allies with the us...   \n",
       "1902193          0      0  dear young philip  general kelly can speak for...   \n",
       "\n",
       "                                                    tokens  sample_weight  \n",
       "0        [this, is, so, cool, it, s, like, would, you, ...       0.808275  \n",
       "1        [thank, you, this, would, make, my, life, a, l...       0.808275  \n",
       "2        [this, is, such, an, urgent, design, problem, ...       0.808275  \n",
       "3        [is, this, something, i, ll, be, able, to, ins...       0.808275  \n",
       "4               [haha, you, guy, are, a, bunch, of, loser]       1.616550  \n",
       "...                                                    ...            ...  \n",
       "1902189  [that, s, the, thing, it, s, called, a, plan, ...       0.808275  \n",
       "1902190  [it, s, not, quite, the, way, you, describe, i...       0.808275  \n",
       "1902191  [what, right, have, you, to, criticise, you, h...       0.808275  \n",
       "1902192  [my, concern, is, that, if, china, ally, with,...       0.808275  \n",
       "1902193  [dear, young, philip, general, kelly, can, spe...       0.808275  \n",
       "\n",
       "[1902194 rows x 8 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample weights\n",
    "# There are four groups\n",
    "# Identity-Toxic\n",
    "# Identity-Non toxic\n",
    "# No identity - Toxic\n",
    "# No identity - non toxic\n",
    "# we want equal representation of each of these groups \n",
    "# Let us see how many belong to each group \n",
    "# Identity and toxic\n",
    "\n",
    "# identity_toxic = len(data[(data.identity == 1) & (data.label == 1)])\n",
    "# identity_nontoxic = len(data[(data.identity == 1) & (data.label == 0)])\n",
    "# nonidentity_toxic = len(data[(data.identity == 0) & (data.label == 1)])\n",
    "# nonidentity_nontoxic = len(data[(data.identity == 0) & (data.label == 0)])\n",
    "# print(\"Identity and toxic=\", identity_toxic)\n",
    "# print(\"Identity and non-toxic=\", identity_nontoxic)\n",
    "# print(\"Non-identity and toxic=\", nonidentity_toxic)\n",
    "# print(\"Non-identity and non-toxic=\", nonidentity_nontoxic)\n",
    "\n",
    "# sample_weight_identity_toxic = len(data)/identity_toxic/4\n",
    "# sample_weight_identity_nontoxic = len(data)/identity_nontoxic/4\n",
    "# sample_weight_nonidentity_toxic = len(data)/nonidentity_toxic/4\n",
    "# sample_weight_nonidentity_nontoxic = len(data)/nonidentity_nontoxic/4\n",
    "\n",
    "# conditions = [\n",
    "#     (data.identity == 1) & (data.label == 1),\n",
    "#     (data.identity == 1) & (data.label == 0),\n",
    "#     (data.identity == 0) & (data.label == 1),\n",
    "#     (data.identity == 0) & (data.label == 0)\n",
    "# ]\n",
    "\n",
    "# values = [sample_weight_identity_toxic, \n",
    "#             sample_weight_identity_nontoxic, \n",
    "#             sample_weight_nonidentity_toxic, \n",
    "#             sample_weight_nonidentity_nontoxic\n",
    "#         ]\n",
    "\n",
    "# data['sample_weight'] = np.select(conditions, values)\n",
    "# values\n",
    "\n",
    "\n",
    "#Subgroup: Add all the values of the identities along rows\n",
    "#Background Positive, Subgroup Negative - Add all values of targets*~identity\n",
    "# #Background Positive, Subgroup Negative -  Add all values ~targets*identity\n",
    "data[\"sample_weight\"] = np.ones((len(data),)) + \\\n",
    "    (data[\"identity\"].fillna(0)) + \\\n",
    "        (data['label'] * (~data[\"identity\"].astype(bool)).astype(int)) + \\\n",
    "            ((~data[\"label\"].astype(bool)).astype(int) * data[\"identity\"])\n",
    "\n",
    "data[\"sample_weight\"]  /= data[\"sample_weight\"] .mean()\n",
    "data\n",
    "\n",
    "\n",
    "# # #Overall\n",
    "# data[\"sample_weight\"] = np.ones((len(data),))\n",
    "\n",
    "# #Subgroup: Add all the values of the identities along rows\n",
    "# data[\"sample_weight\"] += (data[\"identity\"].fillna(0))\n",
    "\n",
    "# #Background Positive, Subgroup Negative - Add all values of targets*~identity\n",
    "# data[\"sample_weight\"] += data['label'] * (~data[\"identity\"].astype(bool)).astype(int)\n",
    "\n",
    "# # #Background Positive, Subgroup Negative -  Add all values ~targets*identity\n",
    "# data[\"sample_weight\"] += (~data[\"label\"].astype(bool)).astype(int) * data[\"identity\"]\n",
    "\n",
    "# data[\"sample_weight\"]  /= data[\"sample_weight\"] .mean()\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identity</th>\n",
       "      <th>label</th>\n",
       "      <th>sample_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.424826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.424826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.424826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.424826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.424826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804848</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.424826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804850</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.424826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804852</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.424826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804855</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.424826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804858</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.424826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142341 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         identity  label  sample_weight\n",
       "11              1      0       2.424826\n",
       "68              1      0       2.424826\n",
       "185             1      0       2.424826\n",
       "191             1      0       2.424826\n",
       "197             1      0       2.424826\n",
       "...           ...    ...            ...\n",
       "1804848         1      0       2.424826\n",
       "1804850         1      0       2.424826\n",
       "1804852         1      0       2.424826\n",
       "1804855         1      0       2.424826\n",
       "1804858         1      0       2.424826\n",
       "\n",
       "[142341 rows x 3 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[[\"identity\", \"label\", \"sample_weight\"]][(train_data.identity==1) & (train_data.label==0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.808275    1600589\n",
       "1.616550     152005\n",
       "2.424826     149600\n",
       "Name: sample_weight, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data[\"sample_weight\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1902194"
      ]
     },
     "execution_count": 637,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1902194, 8)"
      ]
     },
     "execution_count": 636,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1902194"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[data.train_set==1]\n",
    "test_data = data[data.train_set==0]\n",
    "\n",
    "# train_data.to_csv(\"train_set.csv\")\n",
    "# test_data.to_csv(\"test_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    1804874\n",
       "Name: tokens, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.tokens.isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>identity</th>\n",
       "      <th>train_set</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sample_weight</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1750189</td>\n",
       "      <td>1750189</td>\n",
       "      <td>1750189</td>\n",
       "      <td>1750189</td>\n",
       "      <td>1750189</td>\n",
       "      <td>1750189</td>\n",
       "      <td>1750189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>152005</td>\n",
       "      <td>152005</td>\n",
       "      <td>152005</td>\n",
       "      <td>152005</td>\n",
       "      <td>152005</td>\n",
       "      <td>152005</td>\n",
       "      <td>152005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  comment_text  identity  train_set  clean_text   tokens  \\\n",
       "label                                                                    \n",
       "0      1750189       1750189   1750189    1750189     1750189  1750189   \n",
       "1       152005        152005    152005     152005      152005   152005   \n",
       "\n",
       "       sample_weight  \n",
       "label                 \n",
       "0            1750189  \n",
       "1             152005  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby([\"label\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>identity</th>\n",
       "      <th>train_set</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sample_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>59863</td>\n",
       "      <td>FFFFUUUUUUUUUUUUUUU</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>ffffuuuuuuuuuuuuuuu</td>\n",
       "      <td>[ffffuuuuuuuuuuuuuuu]</td>\n",
       "      <td>0.808275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>239616</td>\n",
       "      <td>Awesome!</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>awesome</td>\n",
       "      <td>[awesome]</td>\n",
       "      <td>0.808275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>240316</td>\n",
       "      <td>Perhaps!</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>perhaps</td>\n",
       "      <td>[perhaps]</td>\n",
       "      <td>0.808275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>240431</td>\n",
       "      <td>log</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>log</td>\n",
       "      <td>[log]</td>\n",
       "      <td>0.808275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>240481</td>\n",
       "      <td>101th!</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>101th</td>\n",
       "      <td>[101th]</td>\n",
       "      <td>0.808275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804385</th>\n",
       "      <td>6333176</td>\n",
       "      <td>Nonsense</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>nonsense</td>\n",
       "      <td>[nonsense]</td>\n",
       "      <td>0.808275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804413</th>\n",
       "      <td>6333213</td>\n",
       "      <td>prolly!</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>prolly</td>\n",
       "      <td>[prolly]</td>\n",
       "      <td>0.808275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804606</th>\n",
       "      <td>6333483</td>\n",
       "      <td>How?</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>how</td>\n",
       "      <td>[how]</td>\n",
       "      <td>0.808275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804654</th>\n",
       "      <td>6333556</td>\n",
       "      <td>Exactly.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>exactly</td>\n",
       "      <td>[exactly]</td>\n",
       "      <td>0.808275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804676</th>\n",
       "      <td>6333602</td>\n",
       "      <td>Whoosh!</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>whoosh</td>\n",
       "      <td>[whoosh]</td>\n",
       "      <td>0.808275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11785 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id         comment_text  identity  train_set  label  \\\n",
       "7          59863  FFFFUUUUUUUUUUUUUUU         0          1      0   \n",
       "35        239616             Awesome!         0          1      0   \n",
       "294       240316             Perhaps!         0          1      0   \n",
       "375       240431                  log         0          1      0   \n",
       "413       240481               101th!         0          1      0   \n",
       "...          ...                  ...       ...        ...    ...   \n",
       "1804385  6333176             Nonsense         0          1      0   \n",
       "1804413  6333213              prolly!         0          1      0   \n",
       "1804606  6333483                 How?         0          1      0   \n",
       "1804654  6333556             Exactly.         0          1      0   \n",
       "1804676  6333602              Whoosh!         0          1      0   \n",
       "\n",
       "                  clean_text                 tokens  sample_weight  \n",
       "7        ffffuuuuuuuuuuuuuuu  [ffffuuuuuuuuuuuuuuu]       0.808275  \n",
       "35                  awesome               [awesome]       0.808275  \n",
       "294                 perhaps               [perhaps]       0.808275  \n",
       "375                      log                  [log]       0.808275  \n",
       "413                   101th                 [101th]       0.808275  \n",
       "...                      ...                    ...            ...  \n",
       "1804385             nonsense             [nonsense]       0.808275  \n",
       "1804413              prolly                [prolly]       0.808275  \n",
       "1804606                 how                   [how]       0.808275  \n",
       "1804654             exactly               [exactly]       0.808275  \n",
       "1804676              whoosh                [whoosh]       0.808275  \n",
       "\n",
       "[11785 rows x 8 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[train_data[\"tokens\"].apply(lambda x: len(x)==1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing rows from train set that have zero tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>identity</th>\n",
       "      <th>train_set</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sample_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>this is so cool  it s like   would you want yo...</td>\n",
       "      <td>[this, is, so, cool, it, s, like, would, you, ...</td>\n",
       "      <td>0.808275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>thank you  this would make my life a lot less ...</td>\n",
       "      <td>[thank, you, this, would, make, my, life, a, l...</td>\n",
       "      <td>0.808275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59852</td>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>this is such an urgent design problem  kudos t...</td>\n",
       "      <td>[this, is, such, an, urgent, design, problem, ...</td>\n",
       "      <td>0.808275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59855</td>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>is this something i ll be able to install on m...</td>\n",
       "      <td>[is, this, something, i, ll, be, able, to, ins...</td>\n",
       "      <td>0.808275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59856</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>haha you guys are a bunch of losers</td>\n",
       "      <td>[haha, you, guy, are, a, bunch, of, loser]</td>\n",
       "      <td>1.616550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804869</th>\n",
       "      <td>6333967</td>\n",
       "      <td>Maybe the tax on \"things\" would be collected w...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>maybe the tax on  things  would be collected w...</td>\n",
       "      <td>[maybe, the, tax, on, thing, would, be, collec...</td>\n",
       "      <td>0.808275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804870</th>\n",
       "      <td>6333969</td>\n",
       "      <td>What do you call people who STILL think the di...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>what do you call people who still think the di...</td>\n",
       "      <td>[what, do, you, call, people, who, still, thin...</td>\n",
       "      <td>0.808275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804871</th>\n",
       "      <td>6333982</td>\n",
       "      <td>thank you ,,,right or wrong,,, i am following ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>thank you  right or wrong  i am following your...</td>\n",
       "      <td>[thank, you, right, or, wrong, i, am, followin...</td>\n",
       "      <td>0.808275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804872</th>\n",
       "      <td>6334009</td>\n",
       "      <td>Anyone who is quoted as having the following e...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>anyone who is quoted as having the following e...</td>\n",
       "      <td>[anyone, who, is, quoted, a, having, the, foll...</td>\n",
       "      <td>1.616550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804873</th>\n",
       "      <td>6334010</td>\n",
       "      <td>Students defined as EBD are legally just as di...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>students defined as ebd are legally just as di...</td>\n",
       "      <td>[student, defined, a, ebd, are, legally, just,...</td>\n",
       "      <td>0.808275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1804654 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                       comment_text  identity  \\\n",
       "0          59848  This is so cool. It's like, 'would you want yo...         0   \n",
       "1          59849  Thank you!! This would make my life a lot less...         0   \n",
       "2          59852  This is such an urgent design problem; kudos t...         0   \n",
       "3          59855  Is this something I'll be able to install on m...         0   \n",
       "4          59856               haha you guys are a bunch of losers.         0   \n",
       "...          ...                                                ...       ...   \n",
       "1804869  6333967  Maybe the tax on \"things\" would be collected w...         0   \n",
       "1804870  6333969  What do you call people who STILL think the di...         0   \n",
       "1804871  6333982  thank you ,,,right or wrong,,, i am following ...         0   \n",
       "1804872  6334009  Anyone who is quoted as having the following e...         0   \n",
       "1804873  6334010  Students defined as EBD are legally just as di...         0   \n",
       "\n",
       "         train_set  label                                         clean_text  \\\n",
       "0                1      0  this is so cool  it s like   would you want yo...   \n",
       "1                1      0  thank you  this would make my life a lot less ...   \n",
       "2                1      0  this is such an urgent design problem  kudos t...   \n",
       "3                1      0  is this something i ll be able to install on m...   \n",
       "4                1      1               haha you guys are a bunch of losers    \n",
       "...            ...    ...                                                ...   \n",
       "1804869          1      0  maybe the tax on  things  would be collected w...   \n",
       "1804870          1      0  what do you call people who still think the di...   \n",
       "1804871          1      0  thank you  right or wrong  i am following your...   \n",
       "1804872          1      1  anyone who is quoted as having the following e...   \n",
       "1804873          1      0  students defined as ebd are legally just as di...   \n",
       "\n",
       "                                                    tokens  sample_weight  \n",
       "0        [this, is, so, cool, it, s, like, would, you, ...       0.808275  \n",
       "1        [thank, you, this, would, make, my, life, a, l...       0.808275  \n",
       "2        [this, is, such, an, urgent, design, problem, ...       0.808275  \n",
       "3        [is, this, something, i, ll, be, able, to, ins...       0.808275  \n",
       "4               [haha, you, guy, are, a, bunch, of, loser]       1.616550  \n",
       "...                                                    ...            ...  \n",
       "1804869  [maybe, the, tax, on, thing, would, be, collec...       0.808275  \n",
       "1804870  [what, do, you, call, people, who, still, thin...       0.808275  \n",
       "1804871  [thank, you, right, or, wrong, i, am, followin...       0.808275  \n",
       "1804872  [anyone, who, is, quoted, a, having, the, foll...       1.616550  \n",
       "1804873  [student, defined, a, ebd, are, legally, just,...       0.808275  \n",
       "\n",
       "[1804654 rows x 8 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = train_data[train_data[\"tokens\"].apply(lambda x: len(x)!=0)]\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>identity</th>\n",
       "      <th>train_set</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sample_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59856</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>haha you guys are a bunch of losers</td>\n",
       "      <td>[haha, you, guy, are, a, bunch, of, loser]</td>\n",
       "      <td>1.61655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>59859</td>\n",
       "      <td>ur a sh*tty comment.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>ur a sh tty comment</td>\n",
       "      <td>[ur, a, sh, tty, comment]</td>\n",
       "      <td>1.61655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>239583</td>\n",
       "      <td>It's ridiculous that these guys are being call...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>it s ridiculous that these guys are being call...</td>\n",
       "      <td>[it, s, ridiculous, that, these, guy, are, bei...</td>\n",
       "      <td>1.61655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>239584</td>\n",
       "      <td>This story gets more ridiculous by the hour! A...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>this story gets more ridiculous by the hour  a...</td>\n",
       "      <td>[this, story, get, more, ridiculous, by, the, ...</td>\n",
       "      <td>1.61655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>239592</td>\n",
       "      <td>Angry trolls, misogynists and Racists\", oh my....</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>angry trolls  misogynists and racists  oh my  ...</td>\n",
       "      <td>[angry, troll, misogynist, and, racist, oh, my...</td>\n",
       "      <td>1.61655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804825</th>\n",
       "      <td>6333841</td>\n",
       "      <td>Who is the jerk in the last row between the C ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>who is the jerk in the last row between the c ...</td>\n",
       "      <td>[who, is, the, jerk, in, the, last, row, betwe...</td>\n",
       "      <td>1.61655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804838</th>\n",
       "      <td>6333871</td>\n",
       "      <td>Nobody really cares that she supports the scie...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>nobody really cares that she supports the scie...</td>\n",
       "      <td>[nobody, really, care, that, she, support, the...</td>\n",
       "      <td>1.61655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804856</th>\n",
       "      <td>6333923</td>\n",
       "      <td>Believing in God or not believing in God are p...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>believing in god or not believing in god are p...</td>\n",
       "      <td>[believing, in, god, or, not, believing, in, g...</td>\n",
       "      <td>1.61655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804857</th>\n",
       "      <td>6333927</td>\n",
       "      <td>I take your point, but I think you're shooting...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>i take your point  but i think you re shooting...</td>\n",
       "      <td>[i, take, your, point, but, i, think, you, re,...</td>\n",
       "      <td>1.61655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804872</th>\n",
       "      <td>6334009</td>\n",
       "      <td>Anyone who is quoted as having the following e...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>anyone who is quoted as having the following e...</td>\n",
       "      <td>[anyone, who, is, quoted, a, having, the, foll...</td>\n",
       "      <td>1.61655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144334 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                       comment_text  identity  \\\n",
       "4          59856               haha you guys are a bunch of losers.         0   \n",
       "5          59859                               ur a sh*tty comment.         0   \n",
       "13        239583  It's ridiculous that these guys are being call...         0   \n",
       "14        239584  This story gets more ridiculous by the hour! A...         0   \n",
       "19        239592  Angry trolls, misogynists and Racists\", oh my....         0   \n",
       "...          ...                                                ...       ...   \n",
       "1804825  6333841  Who is the jerk in the last row between the C ...         0   \n",
       "1804838  6333871  Nobody really cares that she supports the scie...         0   \n",
       "1804856  6333923  Believing in God or not believing in God are p...         1   \n",
       "1804857  6333927  I take your point, but I think you're shooting...         0   \n",
       "1804872  6334009  Anyone who is quoted as having the following e...         0   \n",
       "\n",
       "         train_set  label                                         clean_text  \\\n",
       "4                1      1               haha you guys are a bunch of losers    \n",
       "5                1      1                               ur a sh tty comment    \n",
       "13               1      1  it s ridiculous that these guys are being call...   \n",
       "14               1      1  this story gets more ridiculous by the hour  a...   \n",
       "19               1      1  angry trolls  misogynists and racists  oh my  ...   \n",
       "...            ...    ...                                                ...   \n",
       "1804825          1      1  who is the jerk in the last row between the c ...   \n",
       "1804838          1      1  nobody really cares that she supports the scie...   \n",
       "1804856          1      1  believing in god or not believing in god are p...   \n",
       "1804857          1      1  i take your point  but i think you re shooting...   \n",
       "1804872          1      1  anyone who is quoted as having the following e...   \n",
       "\n",
       "                                                    tokens  sample_weight  \n",
       "4               [haha, you, guy, are, a, bunch, of, loser]        1.61655  \n",
       "5                                [ur, a, sh, tty, comment]        1.61655  \n",
       "13       [it, s, ridiculous, that, these, guy, are, bei...        1.61655  \n",
       "14       [this, story, get, more, ridiculous, by, the, ...        1.61655  \n",
       "19       [angry, troll, misogynist, and, racist, oh, my...        1.61655  \n",
       "...                                                    ...            ...  \n",
       "1804825  [who, is, the, jerk, in, the, last, row, betwe...        1.61655  \n",
       "1804838  [nobody, really, care, that, she, support, the...        1.61655  \n",
       "1804856  [believing, in, god, or, not, believing, in, g...        1.61655  \n",
       "1804857  [i, take, your, point, but, i, think, you, re,...        1.61655  \n",
       "1804872  [anyone, who, is, quoted, a, having, the, foll...        1.61655  \n",
       "\n",
       "[144334 rows x 8 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>identity</th>\n",
       "      <th>train_set</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sample_weight</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>144448</td>\n",
       "      <td>144448</td>\n",
       "      <td>144448</td>\n",
       "      <td>144448</td>\n",
       "      <td>144448</td>\n",
       "      <td>144448</td>\n",
       "      <td>144448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>144334</td>\n",
       "      <td>144334</td>\n",
       "      <td>144334</td>\n",
       "      <td>144334</td>\n",
       "      <td>144334</td>\n",
       "      <td>144334</td>\n",
       "      <td>144334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  comment_text  identity  train_set  clean_text  tokens  \\\n",
       "label                                                                  \n",
       "0      144448        144448    144448     144448      144448  144448   \n",
       "1      144334        144334    144334     144334      144334  144334   \n",
       "\n",
       "       sample_weight  \n",
       "label                 \n",
       "0             144448  \n",
       "1             144334  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No Identity and toxix: 116371\n",
    "# No Identity and non-toxic: 1517979\n",
    "# identity and toxic: 27963 \n",
    "# identity and non-toxic: 142341 \n",
    "# total train data size: 1804654 \n",
    "# total non toxic comments: 1660320\n",
    "# total toxic comments: 144334\n",
    "\n",
    "# # Let us under-sample the first three groups\n",
    "# train_data1 = train_data[(train_data.identity==0) & (train_data.label==1)].sample(frac=0.30, random_state=0)\n",
    "# train_data2 = train_data[(train_data.identity==0) & (train_data.label==0)].sample(frac=0.02, random_state=0)\n",
    "# train_data3 = train_data[(train_data.identity==1) & (train_data.label==1)]\n",
    "# train_data4 = train_data[(train_data.identity==1) & (train_data.label==0)]\n",
    "\n",
    "# Let us undersample the non-toxic group\n",
    "train_data1 = train_data[train_data.label==0].sample(frac=0.087, random_state=0)\n",
    "train_data2 = train_data[train_data.label==1]\n",
    "\n",
    "train_data = train_data1.append(train_data2)\n",
    "train_data.groupby([\"label\"]).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>identity</th>\n",
       "      <th>train_set</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sample_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1061654</th>\n",
       "      <td>5414429</td>\n",
       "      <td>Stupid Politics is stupid no matter what side ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>stupid politics is stupid no matter what side ...</td>\n",
       "      <td>[stupid, politics, is, stupid, no, matter, wha...</td>\n",
       "      <td>1.616550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134930</th>\n",
       "      <td>406832</td>\n",
       "      <td>Outsourcing to the L48 is where the major gang...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>outsourcing to the l48 is where the major gang...</td>\n",
       "      <td>[outsourcing, to, the, l48, is, where, the, ma...</td>\n",
       "      <td>1.616550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949651</th>\n",
       "      <td>5280417</td>\n",
       "      <td>What's that old saying...\" Be careful what wis...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>what s that old saying  be careful what wish f...</td>\n",
       "      <td>[what, s, that, old, saying, be, careful, what...</td>\n",
       "      <td>1.616550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626163</th>\n",
       "      <td>1008214</td>\n",
       "      <td>You don't have to be smart to be dangerous. Mo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>you don t have to be smart to be dangerous  mo...</td>\n",
       "      <td>[you, don, t, have, to, be, smart, to, be, dan...</td>\n",
       "      <td>1.616550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267603</th>\n",
       "      <td>5664056</td>\n",
       "      <td>Trump as President.....it is to laugh.   The w...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>trump as president it is to laugh    the whole...</td>\n",
       "      <td>[trump, a, president, it, is, to, laugh, the, ...</td>\n",
       "      <td>1.616550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804848</th>\n",
       "      <td>6333897</td>\n",
       "      <td>Women's rights?  Last I checked women were jus...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>women s rights   last i checked women were jus...</td>\n",
       "      <td>[woman, s, right, last, i, checked, woman, wer...</td>\n",
       "      <td>2.424826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804850</th>\n",
       "      <td>6333907</td>\n",
       "      <td>Every time there are testimonies, like this, I...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>every time there are testimonies  like this  i...</td>\n",
       "      <td>[every, time, there, are, testimony, like, thi...</td>\n",
       "      <td>2.424826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804852</th>\n",
       "      <td>6333915</td>\n",
       "      <td>Xi and his comrades must be smirking over Trum...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>xi and his comrades must be smirking over trum...</td>\n",
       "      <td>[xi, and, his, comrade, must, be, smirking, ov...</td>\n",
       "      <td>2.424826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804855</th>\n",
       "      <td>6333920</td>\n",
       "      <td>It is of course normal and natural for Eugene ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>it is of course normal and natural for eugene ...</td>\n",
       "      <td>[it, is, of, course, normal, and, natural, for...</td>\n",
       "      <td>2.424826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804858</th>\n",
       "      <td>6333928</td>\n",
       "      <td>My thought exactly.  The only people he hasn't...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>my thought exactly   the only people he hasn t...</td>\n",
       "      <td>[my, thought, exactly, the, only, people, he, ...</td>\n",
       "      <td>2.424826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>235579 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                       comment_text  identity  \\\n",
       "1061654  5414429  Stupid Politics is stupid no matter what side ...         0   \n",
       "134930    406832  Outsourcing to the L48 is where the major gang...         0   \n",
       "949651   5280417  What's that old saying...\" Be careful what wis...         0   \n",
       "626163   1008214  You don't have to be smart to be dangerous. Mo...         0   \n",
       "1267603  5664056  Trump as President.....it is to laugh.   The w...         0   \n",
       "...          ...                                                ...       ...   \n",
       "1804848  6333897  Women's rights?  Last I checked women were jus...         1   \n",
       "1804850  6333907  Every time there are testimonies, like this, I...         1   \n",
       "1804852  6333915  Xi and his comrades must be smirking over Trum...         1   \n",
       "1804855  6333920  It is of course normal and natural for Eugene ...         1   \n",
       "1804858  6333928  My thought exactly.  The only people he hasn't...         1   \n",
       "\n",
       "         train_set  label                                         clean_text  \\\n",
       "1061654          1      1  stupid politics is stupid no matter what side ...   \n",
       "134930           1      1  outsourcing to the l48 is where the major gang...   \n",
       "949651           1      1  what s that old saying  be careful what wish f...   \n",
       "626163           1      1  you don t have to be smart to be dangerous  mo...   \n",
       "1267603          1      1  trump as president it is to laugh    the whole...   \n",
       "...            ...    ...                                                ...   \n",
       "1804848          1      0  women s rights   last i checked women were jus...   \n",
       "1804850          1      0  every time there are testimonies  like this  i...   \n",
       "1804852          1      0  xi and his comrades must be smirking over trum...   \n",
       "1804855          1      0  it is of course normal and natural for eugene ...   \n",
       "1804858          1      0  my thought exactly   the only people he hasn t...   \n",
       "\n",
       "                                                    tokens  sample_weight  \n",
       "1061654  [stupid, politics, is, stupid, no, matter, wha...       1.616550  \n",
       "134930   [outsourcing, to, the, l48, is, where, the, ma...       1.616550  \n",
       "949651   [what, s, that, old, saying, be, careful, what...       1.616550  \n",
       "626163   [you, don, t, have, to, be, smart, to, be, dan...       1.616550  \n",
       "1267603  [trump, a, president, it, is, to, laugh, the, ...       1.616550  \n",
       "...                                                    ...            ...  \n",
       "1804848  [woman, s, right, last, i, checked, woman, wer...       2.424826  \n",
       "1804850  [every, time, there, are, testimony, like, thi...       2.424826  \n",
       "1804852  [xi, and, his, comrade, must, be, smirking, ov...       2.424826  \n",
       "1804855  [it, is, of, course, normal, and, natural, for...       2.424826  \n",
       "1804858  [my, thought, exactly, the, only, people, he, ...       2.424826  \n",
       "\n",
       "[235579 rows x 8 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>identity</th>\n",
       "      <th>train_set</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sample_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1061654</th>\n",
       "      <td>5414429</td>\n",
       "      <td>Stupid Politics is stupid no matter what side ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>stupid politics is stupid no matter what side ...</td>\n",
       "      <td>[stupid, politics, is, stupid, no, matter, wha...</td>\n",
       "      <td>1.616550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134930</th>\n",
       "      <td>406832</td>\n",
       "      <td>Outsourcing to the L48 is where the major gang...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>outsourcing to the l48 is where the major gang...</td>\n",
       "      <td>[outsourcing, to, the, l48, is, where, the, ma...</td>\n",
       "      <td>1.616550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949651</th>\n",
       "      <td>5280417</td>\n",
       "      <td>What's that old saying...\" Be careful what wis...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>what s that old saying  be careful what wish f...</td>\n",
       "      <td>[what, s, that, old, saying, be, careful, what...</td>\n",
       "      <td>1.616550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626163</th>\n",
       "      <td>1008214</td>\n",
       "      <td>You don't have to be smart to be dangerous. Mo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>you don t have to be smart to be dangerous  mo...</td>\n",
       "      <td>[you, don, t, have, to, be, smart, to, be, dan...</td>\n",
       "      <td>1.616550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267603</th>\n",
       "      <td>5664056</td>\n",
       "      <td>Trump as President.....it is to laugh.   The w...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>trump as president it is to laugh    the whole...</td>\n",
       "      <td>[trump, a, president, it, is, to, laugh, the, ...</td>\n",
       "      <td>1.616550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804848</th>\n",
       "      <td>6333897</td>\n",
       "      <td>Women's rights?  Last I checked women were jus...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>women s rights   last i checked women were jus...</td>\n",
       "      <td>[woman, s, right, last, i, checked, woman, wer...</td>\n",
       "      <td>2.424826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804850</th>\n",
       "      <td>6333907</td>\n",
       "      <td>Every time there are testimonies, like this, I...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>every time there are testimonies  like this  i...</td>\n",
       "      <td>[every, time, there, are, testimony, like, thi...</td>\n",
       "      <td>2.424826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804852</th>\n",
       "      <td>6333915</td>\n",
       "      <td>Xi and his comrades must be smirking over Trum...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>xi and his comrades must be smirking over trum...</td>\n",
       "      <td>[xi, and, his, comrade, must, be, smirking, ov...</td>\n",
       "      <td>2.424826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804855</th>\n",
       "      <td>6333920</td>\n",
       "      <td>It is of course normal and natural for Eugene ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>it is of course normal and natural for eugene ...</td>\n",
       "      <td>[it, is, of, course, normal, and, natural, for...</td>\n",
       "      <td>2.424826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804858</th>\n",
       "      <td>6333928</td>\n",
       "      <td>My thought exactly.  The only people he hasn't...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>my thought exactly   the only people he hasn t...</td>\n",
       "      <td>[my, thought, exactly, the, only, people, he, ...</td>\n",
       "      <td>2.424826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>235579 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                       comment_text  identity  \\\n",
       "1061654  5414429  Stupid Politics is stupid no matter what side ...         0   \n",
       "134930    406832  Outsourcing to the L48 is where the major gang...         0   \n",
       "949651   5280417  What's that old saying...\" Be careful what wis...         0   \n",
       "626163   1008214  You don't have to be smart to be dangerous. Mo...         0   \n",
       "1267603  5664056  Trump as President.....it is to laugh.   The w...         0   \n",
       "...          ...                                                ...       ...   \n",
       "1804848  6333897  Women's rights?  Last I checked women were jus...         1   \n",
       "1804850  6333907  Every time there are testimonies, like this, I...         1   \n",
       "1804852  6333915  Xi and his comrades must be smirking over Trum...         1   \n",
       "1804855  6333920  It is of course normal and natural for Eugene ...         1   \n",
       "1804858  6333928  My thought exactly.  The only people he hasn't...         1   \n",
       "\n",
       "         train_set  label                                         clean_text  \\\n",
       "1061654          1      1  stupid politics is stupid no matter what side ...   \n",
       "134930           1      1  outsourcing to the l48 is where the major gang...   \n",
       "949651           1      1  what s that old saying  be careful what wish f...   \n",
       "626163           1      1  you don t have to be smart to be dangerous  mo...   \n",
       "1267603          1      1  trump as president it is to laugh    the whole...   \n",
       "...            ...    ...                                                ...   \n",
       "1804848          1      0  women s rights   last i checked women were jus...   \n",
       "1804850          1      0  every time there are testimonies  like this  i...   \n",
       "1804852          1      0  xi and his comrades must be smirking over trum...   \n",
       "1804855          1      0  it is of course normal and natural for eugene ...   \n",
       "1804858          1      0  my thought exactly   the only people he hasn t...   \n",
       "\n",
       "                                                    tokens  sample_weight  \n",
       "1061654  [stupid, politics, is, stupid, no, matter, wha...       1.616550  \n",
       "134930   [outsourcing, to, the, l48, is, where, the, ma...       1.616550  \n",
       "949651   [what, s, that, old, saying, be, careful, what...       1.616550  \n",
       "626163   [you, don, t, have, to, be, smart, to, be, dan...       1.616550  \n",
       "1267603  [trump, a, president, it, is, to, laugh, the, ...       1.616550  \n",
       "...                                                    ...            ...  \n",
       "1804848  [woman, s, right, last, i, checked, woman, wer...       2.424826  \n",
       "1804850  [every, time, there, are, testimony, like, thi...       2.424826  \n",
       "1804852  [xi, and, his, comrade, must, be, smirking, ov...       2.424826  \n",
       "1804855  [it, is, of, course, normal, and, natural, for...       2.424826  \n",
       "1804858  [my, thought, exactly, the, only, people, he, ...       2.424826  \n",
       "\n",
       "[235579 rows x 8 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of class after under-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>identity</th>\n",
       "      <th>train_set</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sample_weight</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138161</td>\n",
       "      <td>138161</td>\n",
       "      <td>138161</td>\n",
       "      <td>138161</td>\n",
       "      <td>138161</td>\n",
       "      <td>138161</td>\n",
       "      <td>138161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50302</td>\n",
       "      <td>50302</td>\n",
       "      <td>50302</td>\n",
       "      <td>50302</td>\n",
       "      <td>50302</td>\n",
       "      <td>50302</td>\n",
       "      <td>50302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  comment_text  identity  train_set  clean_text  tokens  \\\n",
       "label                                                                  \n",
       "0      138161        138161    138161     138161      138161  138161   \n",
       "1       50302         50302     50302      50302       50302   50302   \n",
       "\n",
       "       sample_weight  \n",
       "label                 \n",
       "0             138161  \n",
       "1              50302  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.groupby([\"label\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# undersample = random.sample(, random_state=0)\n",
    "\n",
    "# train_data = undersample.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From henceforth, I have forked \"final_project_sa.ipynb\" to run the Deep Neural Network models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext\n",
    "\n",
    "from collections import Counter\n",
    "from torchtext.vocab import vocab\n",
    "\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.10.2+cu113', '0.11.2')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the library versions\n",
    "torch.__version__, torchtext.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda.\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "from torchtext.vocab import Vectors\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "if USE_CUDA:\n",
    "    DEVICE = torch.device('cuda')\n",
    "    print(\"Using cuda.\")\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "    print(\"Using cpu.\")\n",
    "\n",
    "random.seed(30255)\n",
    "np.random.seed(30255)\n",
    "torch.manual_seed(30255)\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(30255)\n",
    "\n",
    "# # Change the following to false when training on\n",
    "# # the full set\n",
    "# DEVELOPING = True    \n",
    "# # DEVELOPING = False\n",
    "\n",
    "# if DEVELOPING:\n",
    "#     print('Small development version')\n",
    "#     BATCH_SIZE = 4\n",
    "#     EMBEDDING_SIZE = 20\n",
    "#     MAX_VOCAB_SIZE = 5000\n",
    "#     TRAIN_DATA_SET = \"train_set.csv\"\n",
    "#     TEST_DATA_SET = \"test_set.csv\"\n",
    "#     BPTT_LENGTH = 8\n",
    "# else:\n",
    "#     print('Full version')\n",
    "#     BATCH_SIZE = 32\n",
    "#     EMBEDDING_SIZE = 650\n",
    "#     MAX_VOCAB_SIZE = 50000\n",
    "#     TRAIN_DATA_SET = \"train_set.csv\"\n",
    "#     TEST_DATA_SET = \"test_set.csv\"\n",
    "#     BPTT_LENGTH = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE = GloVe(name='6B', dim=100)\n",
    "FASTTEXT = FastText('simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = list(train_data[[\"tokens\", \"label\", \"sample_weight\"]].to_records(index=False))\n",
    "val = list(val_data[[\"tokens\", \"label\", \"sample_weight\"]].to_records(index=False))\n",
    "test = list(test_data[[\"tokens\", \"label\", \"sample_weight\"]].to_records(index=False))\n",
    "# train_target = train_data[\"label\"]\n",
    "# val_target = val_data[\"label\"]\n",
    "# test_target = test_data[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Vocab (Word-to-Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXT = torchtext.legacy.data.Field(lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import Vocab\n",
    "\n",
    "counter_words = Counter()\n",
    "for tokens, _, _ in train:\n",
    "    counter_words.update(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100261"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(counter_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 100263\n"
     ]
    }
   ],
   "source": [
    "# from torchtext.vocab import vocab\n",
    "\n",
    "# vocab_words = vocab(counter_words)\n",
    "# vocab_words.set_default_index(0)\n",
    "\n",
    "# vocab_words = torchtext.vocab.build_vocab_from_iterator(counter_words, specials=[\"<unk>\", \"<pad>\"], special_first=True)\n",
    "\n",
    "from torchtext.legacy.vocab import Vocab as retired_vocab\n",
    "\n",
    "vocab_words = retired_vocab(counter_words, specials=[\"<pad>\", \"<unk>\"], specials_first=True)\n",
    "\n",
    "print(\"Number of unique words:\", len(vocab_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_words.itos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the collate function that converts the batch data to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# def collate_fn(batch):\n",
    "    \n",
    "#     ## WRITE YOUR CODE BELOW\n",
    "#     word_idxs = []\n",
    "#     labels = []\n",
    "#     for tokens, label in batch:\n",
    "#         word_idxs.append(vocab_words.lookup_indices(tokens))\n",
    "#         labels.append(label)\n",
    "#     word_idxs = torch.tensor(word_idxs, dtype=torch.int32)\n",
    "#     word_idxs = word_idxs.T\n",
    "\n",
    "#     # word_idxs = pad_sequence([tokens for tokens, _ in batch], batch_first=False)\n",
    "#     labels = torch.tensor(labels, dtype=torch.int32)    \n",
    "    \n",
    "#     return word_idxs.to(device), labels.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pack_sequence, pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \n",
    "    ## WRITE YOUR CODE BELOW\n",
    "    word_idxs = []\n",
    "    labels = []\n",
    "    sample_weights=[]\n",
    "    for tokens, label, sample_weight in batch:\n",
    "        # print(\"Tokens=\", tokens)\n",
    "        # print(\"Labels=\", label)\n",
    "        token_tensor = torch.tensor(vocab_words.lookup_indices(tokens), dtype=torch.int32)\n",
    "        word_idxs.append(token_tensor)\n",
    "        # print(\"List of text_tensors=\", word_idxs)\n",
    "        labels.append(label)\n",
    "        sample_weights.append(sample_weight)\n",
    "    # seq_lengths = torch.LongTensor([v for v in map(len, word_idxs)])\n",
    "    # max_len = max([len(v) for v in word_idxs])\n",
    "\n",
    "    #packing sequence\n",
    "    word_idxs = pad_sequence(word_idxs, batch_first=True)\n",
    "    # print(\"padded text_tensors=\", word_idxs)\n",
    "    # word_idxs = torch.tensor(word_idxs, dtype=torch.int32)\n",
    "    word_idxs = word_idxs.T\n",
    "    # print(\"transposed text_tensors=\", word_idxs)\n",
    "    # word_idxs = pad_sequence([tokens for tokens, _ in batch], batch_first=False)\n",
    "    labels = torch.tensor(labels, dtype=torch.int32) \n",
    "    labels = labels.type(torch.LongTensor)\n",
    "    sample_weights = torch.tensor(labels, dtype=torch.float32) \n",
    "    # print(\"Label tensors\")\n",
    "    \n",
    "    return word_idxs.to(device), labels.to(device), sample_weights.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 1, 6],\n",
       "        [5, 2, 0],\n",
       "        [0, 3, 0]])"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# b = torch.tensor([1,2,3])\n",
    "# a = torch.tensor([4,5])\n",
    "# c = torch.tensor([6])\n",
    "# pad_sequence([a,b,c], batch_first=True).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64 # batch size for training\n",
    "\n",
    "train_dataloader = DataLoader(train, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, \n",
    "                              collate_fn=collate_fn)\n",
    "valid_dataloader = DataLoader(val, batch_size=BATCH_SIZE,\n",
    "                              shuffle=False, \n",
    "                              collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test, batch_size=BATCH_SIZE,\n",
    "                             shuffle=False, \n",
    "                             collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataloader = DataLoader(a, batch_size=BATCH_SIZE,\n",
    "#                               shuffle=True, \n",
    "#                               collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "304"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max_length = 0\n",
    "# length_list = []\n",
    "# for sequence, label in train:\n",
    "#     current_length = len(sequence)\n",
    "#     length_list.append(current_length)\n",
    "#     if current_length > max_length:\n",
    "#         max_length = current_length\n",
    "\n",
    "# max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36098"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36097.46875"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1155119/32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import vocab\n",
    "\n",
    "fasttext = vocab.FastText('simple')\n",
    "fasttext_vectors = fasttext.get_vecs_by_tokens(vocab_words.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100263, 300])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_vectors.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the class for the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class RNNLM(nn.Module):\n",
    "    \"\"\" Container module with an linear encoder/embedding, an RNN module, and a linear decoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, rnn_type, vocab_size, embedding_dim, hidden_dim, num_layers, output_dim, \n",
    "                 dropout=0.5, use_fasttext=False, freeze_fasttext=False, bidirectional=False):\n",
    "        ''' Initialize model parameters corresponding to ---\n",
    "            - embedding layer\n",
    "            - recurrent neural network layer---one of LSTM, GRU, or RNN---with \n",
    "              optionally more than one layer\n",
    "            - linear layer to map from hidden vector to the vocabulary\n",
    "            - optionally, dropout layers.  Dropout layers can be placed after \n",
    "              the embedding layer or/and after the RNN layer. Dropout within\n",
    "              an RNN is only applied when there are two or more num_layers.\n",
    "            - optionally, initialize the model parameters.\n",
    "            \n",
    "            The arguments are:\n",
    "            \n",
    "            rnn_type: One of 'LSTM', 'GRU', 'RNN_TANH', 'RNN_RELU'\n",
    "            vocab_size: size of vocabulary\n",
    "            embedding_dim: size of an embedding vector\n",
    "            hidden_dim: size of hidden/state vector in RNN\n",
    "            num_layers: number of layers in RNN\n",
    "            dropout: dropout probability.\n",
    "            \n",
    "        '''\n",
    "        super(RNNLM, self).__init__()\n",
    "        \n",
    "        if use_fasttext:\n",
    "          self.embedding = nn.Embedding.from_pretrained(fasttext_vectors, freeze=freeze_fasttext)\n",
    "        else:\n",
    "          self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        if rnn_type == \"LSTM\":\n",
    "          self.rnn = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=num_layers, dropout=dropout, bidirectional=bidirectional)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
    "        self.hidden_size = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.D = 1\n",
    "        if bidirectional:\n",
    "          self.D = 2\n",
    "        \n",
    "\n",
    "    def forward(self, input, hidden0=None):\n",
    "        ''' \n",
    "        Run forward propagation for a given minibatch of inputs using\n",
    "        hidden0 as the initial hidden state.\n",
    "\n",
    "        In LSTMs hidden0 = (h_0, c_0). \n",
    "\n",
    "        The output of the RNN includes the hidden vector hiddenn = (h_n, c_n).\n",
    "        Return this as well so that it can be used to initialize the next\n",
    "        batch.\n",
    "        \n",
    "        Unlike previous homework sets do not apply softmax or logsoftmax here, since we'll use\n",
    "        the more efficient CrossEntropyLoss.  See \n",
    "        https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html.\n",
    "        '''\n",
    "        batch_size = input.size(-1)\n",
    "        if not hidden0:\n",
    "          h_n = torch.zeros(self.D * self.num_layers, batch_size, self.hidden_size).cuda()\n",
    "          c_n = torch.zeros(self.D * self.num_layers, batch_size, self.hidden_size).cuda()\n",
    "        else:\n",
    "          h_n, c_n = hidden0\n",
    "        \n",
    "        embeds = self.embedding(input).cuda()\n",
    "        x, (h_n, c_n) = self.rnn(embeds, (h_n, c_n))\n",
    "        x = self.dropout(x)\n",
    "        y = self.linear(x[-1])\n",
    "\n",
    "        return y\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.zeros(2,3,4)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[-1] = torch.tensor([[1., 1., 1., 1.],\n",
    "        [1., 1., 1., 1.],\n",
    "        [1., 1., 1., 1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction format =  tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "Actual format =  tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "Prediction format =  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "Actual format =  tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 1, 0, 0, 0, 0, 0, 0], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.206921019337394"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "b =  torch.tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
    "c =  torch.tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "int((b==c).sum())\n",
    "len(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on the Validator Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "def evaluate(model, data):\n",
    "    '''\n",
    "    Evaluate the model on the given data.\n",
    "    '''\n",
    "    # print(\"Entered evaluation\")\n",
    "    model.eval()\n",
    "    it = iter(data)\n",
    "    total_pred = 0. # Number of target words seen\n",
    "    total_correct_pred = 0. # Loss over all target words\n",
    "    true_positives = 0\n",
    "    predicted_positives = 0\n",
    "    actual_positives = 0\n",
    "    with torch.no_grad():\n",
    "        # No gradients need to be maintained during evaluation\n",
    "        # There are no hidden tensors for the first batch, and so will default to zeros.\n",
    "        # hidden = None \n",
    "        for i, batch in enumerate(it):\n",
    "            ''' Do the following:\n",
    "                - Extract the text and target from the batch, and if using CUDA (essentially, using GPUs), place \n",
    "                  the tensors on cuda, using a commands such as \"text = text.cuda()\".  More details are at\n",
    "                  https://pytorch.org/docs/stable/notes/cuda.html.\n",
    "                - Pass the hidden state vector from output of previous batch as the initial hidden vector for\n",
    "                  the current batch. \n",
    "                - Call forward propagation to get output and final hidden state vector.\n",
    "                - Compute the cross entropy loss\n",
    "                - The loss_fn computes the average loss per target word in the batch.  Count the number of target\n",
    "                  words in the batch (it is usually the same, except for the last batch), and use it to track the \n",
    "                  total count (of target words) and total loss see so far over all batches.\n",
    "            '''\n",
    "            # print(\"Entered batch\")\n",
    "            # print(batch[0].shape)\n",
    "            text, target, sample_weight = batch\n",
    "            if USE_CUDA:\n",
    "                text, target, sample_weight = text.cuda(), target.cuda(), sample_weight.cuda()\n",
    "            # hidden = None\n",
    "            # print(\"Evaluate=\", text.shape, target.shape)\n",
    "            output = model(text)\n",
    "            loss = loss_fn(output.view(-1, output.size(-1)), target.view(-1))\n",
    "            # loss = (loss*sample_weight).mean()\n",
    "            # print(\"Shape of the output=\", output.shape)\n",
    "            pred = F.softmax(output.view(-1, output.size(-1)), dim=1)\n",
    "            pred = pred.argmax(dim=1)\n",
    "            # print(\"Predicted output= \", pred)\n",
    "            # print(\"Output format = \", output.view(-1, output.size(-1)))\n",
    "            # print(\"Prediction format = \", pred)\n",
    "            # print(\"Actual output = \", target.view(-1))\n",
    "\n",
    "            # calculate accuracy      \n",
    "            total_pred += len(pred)\n",
    "            total_correct_pred += int((pred==target.view(-1)).sum())\n",
    "\n",
    "            # calculate precision\n",
    "            true_positives += precision_score(target.view(-1).cpu(), pred.cpu()) * (torch.count_nonzero(pred))\n",
    "            predicted_positives += ((torch.count_nonzero(pred)))\n",
    "            # precision = true_positives/predicted_positives\n",
    "\n",
    "            # calculate recall\n",
    "            # true_positives += (recall_score(target.view(-1).cpu(), pred.cpu()) * (torch.count_nonzero(target.view(-1))))\n",
    "            actual_positives += ((torch.count_nonzero(target.view(-1))))\n",
    "            # recall = true_positives/actual_positives\n",
    "                \n",
    "    accuracy = (total_correct_pred / total_pred)\n",
    "    precision = (true_positives/predicted_positives)\n",
    "    recall = (true_positives/actual_positives)\n",
    "    f1score = (2*precision*recall)/(precision+recall)\n",
    "    print(\"Accuracy=\", accuracy)\n",
    "    print(\"Precision=\", precision)\n",
    "    print(\"Recall=\", recall)\n",
    "    print(\"F1-Score=\", f1score)\n",
    "\n",
    "    # model.train()\n",
    "    return accuracy, precision, recall, f1score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8333, device='cuda:0')"
      ]
     },
     "execution_count": 620,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre = torch.tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
    "        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
    "        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], device='cuda:0')\n",
    "\n",
    "act = torch.tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
    "        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
    "\n",
    "true_positives = (recall_score(act.cpu(), pre.cpu()) * (torch.count_nonzero(act)))\n",
    "actual_positives = (torch.count_nonzero(act))\n",
    "(true_positives/actual_positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 0.8761354664450447\n",
      "Precision= tensor(0.3830, device='cuda:0')\n",
      "Recall= tensor(0.8910, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "val_loss, precision, recall = evaluate(model, valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6., device='cuda:0')"
      ]
     },
     "execution_count": 601,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a =  torch.tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]).cuda()\n",
    "b =  torch.tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
    "        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]).cuda()\n",
    "\n",
    "precision_score(b.cpu(), a.cpu()) * (torch.count_nonzero(a))\n",
    "# torch.count_nonzero(a) + torch.count_nonzero(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils import class_weight\n",
    "\n",
    "# y = list(train_data[[\"label\"]].to_records(index=False))\n",
    "# class_weights=class_weight.compute_class_weight('balanced',np.unique(y),y)\n",
    "# print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6820, 1.8733])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "# class_weights \n",
    "y = train_data[\"label\"].tolist()\n",
    "class_weights=class_weight.compute_class_weight('balanced',np.unique(y),y)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "# print(class_weights[0])\n",
    "# print(class_weights[1])\n",
    "class_weights\n",
    "# class_weights=class_weight.compute_class_weight('balanced',np.unique(y),y)\n",
    "# array([0.54342593, 6.25692904])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5434, 6.2569])"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = Counter(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 1328515, 1: 115384})"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.543425930456186"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1443899/1328515/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2944.734375"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "188463/64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.543425930456186\n",
      "6.256929036954864\n"
     ]
    }
   ],
   "source": [
    "# print(class_weights[0])\n",
    "# print(class_weights[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model: LSTM with randomly initialized embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 100 the loss is 0.705.\n",
      "Accuracy= 0.732978181509466\n",
      "Precision= tensor(0.2000, device='cuda:0')\n",
      "Recall= tensor(0.0002, device='cuda:0')\n",
      "F1-Score= tensor(0.0005, device='cuda:0')\n",
      "At iteration 200 the loss is 0.690.\n",
      "Accuracy= 0.3116775617624586\n",
      "Precision= tensor(0.2747, device='cuda:0')\n",
      "Recall= tensor(0.9633, device='cuda:0')\n",
      "F1-Score= tensor(0.4275, device='cuda:0')\n",
      "At iteration 300 the loss is 0.685.\n",
      "Accuracy= 0.3933907802020545\n",
      "Precision= tensor(0.2926, device='cuda:0')\n",
      "Recall= tensor(0.8979, device='cuda:0')\n",
      "F1-Score= tensor(0.4413, device='cuda:0')\n",
      "At iteration 400 the loss is 0.687.\n",
      "Accuracy= 0.5536972578317344\n",
      "Precision= tensor(0.3330, device='cuda:0')\n",
      "Recall= tensor(0.6705, device='cuda:0')\n",
      "F1-Score= tensor(0.4450, device='cuda:0')\n",
      "At iteration 500 the loss is 0.665.\n",
      "Accuracy= 0.6443670939808133\n",
      "Precision= tensor(0.3656, device='cuda:0')\n",
      "Recall= tensor(0.4528, device='cuda:0')\n",
      "F1-Score= tensor(0.4046, device='cuda:0')\n",
      "At iteration 600 the loss is 0.693.\n",
      "Accuracy= 0.5291196196621105\n",
      "Precision= tensor(0.3265, device='cuda:0')\n",
      "Recall= tensor(0.7197, device='cuda:0')\n",
      "F1-Score= tensor(0.4492, device='cuda:0')\n",
      "At iteration 700 the loss is 0.656.\n",
      "Accuracy= 0.5977162747262077\n",
      "Precision= tensor(0.3488, device='cuda:0')\n",
      "Recall= tensor(0.5857, device='cuda:0')\n",
      "F1-Score= tensor(0.4372, device='cuda:0')\n",
      "At iteration 800 the loss is 0.699.\n",
      "Accuracy= 0.6104720264878173\n",
      "Precision= tensor(0.3696, device='cuda:0')\n",
      "Recall= tensor(0.6518, device='cuda:0')\n",
      "F1-Score= tensor(0.4717, device='cuda:0')\n",
      "At iteration 900 the loss is 0.571.\n",
      "Accuracy= 0.7233423889973682\n",
      "Precision= tensor(0.4809, device='cuda:0')\n",
      "Recall= tensor(0.4631, device='cuda:0')\n",
      "F1-Score= tensor(0.4718, device='cuda:0')\n",
      "At iteration 1000 the loss is 0.649.\n",
      "Accuracy= 0.6661219118770694\n",
      "Precision= tensor(0.4292, device='cuda:0')\n",
      "Recall= tensor(0.7620, device='cuda:0')\n",
      "F1-Score= tensor(0.5491, device='cuda:0')\n",
      "At iteration 1100 the loss is 0.517.\n",
      "Accuracy= 0.6766491213175991\n",
      "Precision= tensor(0.4401, device='cuda:0')\n",
      "Recall= tensor(0.7782, device='cuda:0')\n",
      "F1-Score= tensor(0.5622, device='cuda:0')\n",
      "At iteration 1200 the loss is 0.572.\n",
      "Accuracy= 0.6667374140419391\n",
      "Precision= tensor(0.4337, device='cuda:0')\n",
      "Recall= tensor(0.8139, device='cuda:0')\n",
      "F1-Score= tensor(0.5658, device='cuda:0')\n",
      "At iteration 1300 the loss is 0.539.\n",
      "Accuracy= 0.7786951354104763\n",
      "Precision= tensor(0.5716, device='cuda:0')\n",
      "Recall= tensor(0.6812, device='cuda:0')\n",
      "F1-Score= tensor(0.6216, device='cuda:0')\n",
      "At iteration 1400 the loss is 0.590.\n",
      "Accuracy= 0.7413405212666611\n",
      "Precision= tensor(0.5097, device='cuda:0')\n",
      "Recall= tensor(0.8020, device='cuda:0')\n",
      "F1-Score= tensor(0.6233, device='cuda:0')\n",
      "At iteration 1500 the loss is 0.438.\n",
      "Accuracy= 0.7373503693012989\n",
      "Precision= tensor(0.5049, device='cuda:0')\n",
      "Recall= tensor(0.8015, device='cuda:0')\n",
      "F1-Score= tensor(0.6196, device='cuda:0')\n",
      "At iteration 1600 the loss is 0.597.\n",
      "Accuracy= 0.7768698531284489\n",
      "Precision= tensor(0.5606, device='cuda:0')\n",
      "Recall= tensor(0.7570, device='cuda:0')\n",
      "F1-Score= tensor(0.6442, device='cuda:0')\n",
      "At iteration 1700 the loss is 0.372.\n",
      "Accuracy= 0.7991128279140843\n",
      "Precision= tensor(0.5974, device='cuda:0')\n",
      "Recall= tensor(0.7576, device='cuda:0')\n",
      "F1-Score= tensor(0.6681, device='cuda:0')\n",
      "At iteration 1800 the loss is 0.415.\n",
      "Accuracy= 0.8127387723915442\n",
      "Precision= tensor(0.6225, device='cuda:0')\n",
      "Recall= tensor(0.7575, device='cuda:0')\n",
      "F1-Score= tensor(0.6834, device='cuda:0')\n",
      "At iteration 1900 the loss is 0.428.\n",
      "Accuracy= 0.8130995840054334\n",
      "Precision= tensor(0.6264, device='cuda:0')\n",
      "Recall= tensor(0.7420, device='cuda:0')\n",
      "F1-Score= tensor(0.6794, device='cuda:0')\n",
      "At iteration 2000 the loss is 0.388.\n",
      "Accuracy= 0.8243696408863231\n",
      "Precision= tensor(0.6487, device='cuda:0')\n",
      "Recall= tensor(0.7455, device='cuda:0')\n",
      "F1-Score= tensor(0.6937, device='cuda:0')\n",
      "At iteration 2100 the loss is 0.530.\n",
      "Accuracy= 0.8324136174547925\n",
      "Precision= tensor(0.6682, device='cuda:0')\n",
      "Recall= tensor(0.7389, device='cuda:0')\n",
      "F1-Score= tensor(0.7018, device='cuda:0')\n",
      "At iteration 2200 the loss is 0.407.\n",
      "Accuracy= 0.7948043127599966\n",
      "Precision= tensor(0.5819, device='cuda:0')\n",
      "Recall= tensor(0.8208, device='cuda:0')\n",
      "F1-Score= tensor(0.6810, device='cuda:0')\n",
      "At iteration 2300 the loss is 0.422.\n",
      "Accuracy= 0.8198064351812547\n",
      "Precision= tensor(0.6305, device='cuda:0')\n",
      "Recall= tensor(0.7845, device='cuda:0')\n",
      "F1-Score= tensor(0.6991, device='cuda:0')\n",
      "At iteration 2400 the loss is 0.544.\n",
      "Accuracy= 0.8288479497410646\n",
      "Precision= tensor(0.6492, device='cuda:0')\n",
      "Recall= tensor(0.7801, device='cuda:0')\n",
      "F1-Score= tensor(0.7087, device='cuda:0')\n",
      "At iteration 2500 the loss is 0.484.\n",
      "Accuracy= 0.820379488920961\n",
      "Precision= tensor(0.6277, device='cuda:0')\n",
      "Recall= tensor(0.8031, device='cuda:0')\n",
      "F1-Score= tensor(0.7047, device='cuda:0')\n",
      "At iteration 2600 the loss is 0.307.\n",
      "Accuracy= 0.8000042448425163\n",
      "Precision= tensor(0.5874, device='cuda:0')\n",
      "Recall= tensor(0.8421, device='cuda:0')\n",
      "F1-Score= tensor(0.6920, device='cuda:0')\n",
      "At iteration 2700 the loss is 0.351.\n",
      "Accuracy= 0.8339629849732575\n",
      "Precision= tensor(0.6595, device='cuda:0')\n",
      "Recall= tensor(0.7809, device='cuda:0')\n",
      "F1-Score= tensor(0.7151, device='cuda:0')\n",
      "At iteration 2800 the loss is 0.449.\n",
      "Accuracy= 0.8398420918583921\n",
      "Precision= tensor(0.6760, device='cuda:0')\n",
      "Recall= tensor(0.7678, device='cuda:0')\n",
      "F1-Score= tensor(0.7190, device='cuda:0')\n",
      "At iteration 2900 the loss is 0.386.\n",
      "Accuracy= 0.8223533406910604\n",
      "Precision= tensor(0.6297, device='cuda:0')\n",
      "Recall= tensor(0.8116, device='cuda:0')\n",
      "F1-Score= tensor(0.7091, device='cuda:0')\n",
      "At iteration 100 the loss is 0.297.\n",
      "Accuracy= 0.8520460140928772\n",
      "Precision= tensor(0.7399, device='cuda:0')\n",
      "Recall= tensor(0.6871, device='cuda:0')\n",
      "F1-Score= tensor(0.7125, device='cuda:0')\n",
      "At iteration 200 the loss is 0.282.\n",
      "Accuracy= 0.8177264623482469\n",
      "Precision= tensor(0.6183, device='cuda:0')\n",
      "Recall= tensor(0.8281, device='cuda:0')\n",
      "F1-Score= tensor(0.7080, device='cuda:0')\n",
      "At iteration 300 the loss is 0.451.\n",
      "Accuracy= 0.8469309788606842\n",
      "Precision= tensor(0.7003, device='cuda:0')\n",
      "Recall= tensor(0.7453, device='cuda:0')\n",
      "F1-Score= tensor(0.7221, device='cuda:0')\n",
      "At iteration 400 the loss is 0.529.\n",
      "Accuracy= 0.8315646489515239\n",
      "Precision= tensor(0.6505, device='cuda:0')\n",
      "Recall= tensor(0.7971, device='cuda:0')\n",
      "F1-Score= tensor(0.7163, device='cuda:0')\n",
      "At iteration 500 the loss is 0.356.\n",
      "Accuracy= 0.8181509465998812\n",
      "Precision= tensor(0.6203, device='cuda:0')\n",
      "Recall= tensor(0.8209, device='cuda:0')\n",
      "F1-Score= tensor(0.7067, device='cuda:0')\n",
      "At iteration 600 the loss is 0.265.\n",
      "Accuracy= 0.832074030053485\n",
      "Precision= tensor(0.6510, device='cuda:0')\n",
      "Recall= tensor(0.7991, device='cuda:0')\n",
      "F1-Score= tensor(0.7175, device='cuda:0')\n",
      "At iteration 700 the loss is 0.511.\n",
      "Accuracy= 0.8178750318363188\n",
      "Precision= tensor(0.6173, device='cuda:0')\n",
      "Recall= tensor(0.8353, device='cuda:0')\n",
      "F1-Score= tensor(0.7100, device='cuda:0')\n",
      "At iteration 800 the loss is 0.396.\n",
      "Accuracy= 0.8354274556413958\n",
      "Precision= tensor(0.6597, device='cuda:0')\n",
      "Recall= tensor(0.7914, device='cuda:0')\n",
      "F1-Score= tensor(0.7196, device='cuda:0')\n",
      "At iteration 900 the loss is 0.323.\n",
      "Accuracy= 0.8407971814245692\n",
      "Precision= tensor(0.6748, device='cuda:0')\n",
      "Recall= tensor(0.7786, device='cuda:0')\n",
      "F1-Score= tensor(0.7230, device='cuda:0')\n",
      "At iteration 1000 the loss is 0.353.\n",
      "Accuracy= 0.838292724339927\n",
      "Precision= tensor(0.6625, device='cuda:0')\n",
      "Recall= tensor(0.8032, device='cuda:0')\n",
      "F1-Score= tensor(0.7261, device='cuda:0')\n",
      "At iteration 1100 the loss is 0.408.\n",
      "Accuracy= 0.8343237965871466\n",
      "Precision= tensor(0.6501, device='cuda:0')\n",
      "Recall= tensor(0.8209, device='cuda:0')\n",
      "F1-Score= tensor(0.7256, device='cuda:0')\n",
      "At iteration 1200 the loss is 0.460.\n",
      "Accuracy= 0.808026997198404\n",
      "Precision= tensor(0.5970, device='cuda:0')\n",
      "Recall= tensor(0.8636, device='cuda:0')\n",
      "F1-Score= tensor(0.7059, device='cuda:0')\n",
      "At iteration 1300 the loss is 0.352.\n",
      "Accuracy= 0.8478223957891162\n",
      "Precision= tensor(0.6878, device='cuda:0')\n",
      "Recall= tensor(0.7869, device='cuda:0')\n",
      "F1-Score= tensor(0.7340, device='cuda:0')\n",
      "At iteration 1400 the loss is 0.465.\n",
      "Accuracy= 0.8480983105526785\n",
      "Precision= tensor(0.6872, device='cuda:0')\n",
      "Recall= tensor(0.7906, device='cuda:0')\n",
      "F1-Score= tensor(0.7353, device='cuda:0')\n",
      "At iteration 1500 the loss is 0.425.\n",
      "Accuracy= 0.8445326428389507\n",
      "Precision= tensor(0.6747, device='cuda:0')\n",
      "Recall= tensor(0.8058, device='cuda:0')\n",
      "F1-Score= tensor(0.7345, device='cuda:0')\n",
      "At iteration 1600 the loss is 0.383.\n",
      "Accuracy= 0.839990661346464\n",
      "Precision= tensor(0.6621, device='cuda:0')\n",
      "Recall= tensor(0.8175, device='cuda:0')\n",
      "F1-Score= tensor(0.7317, device='cuda:0')\n",
      "At iteration 1700 the loss is 0.390.\n",
      "Accuracy= 0.8570549282621615\n",
      "Precision= tensor(0.7139, device='cuda:0')\n",
      "Recall= tensor(0.7748, device='cuda:0')\n",
      "F1-Score= tensor(0.7431, device='cuda:0')\n",
      "At iteration 1800 the loss is 0.282.\n",
      "Accuracy= 0.8231810849817471\n",
      "Precision= tensor(0.6227, device='cuda:0')\n",
      "Recall= tensor(0.8559, device='cuda:0')\n",
      "F1-Score= tensor(0.7209, device='cuda:0')\n",
      "At iteration 1900 the loss is 0.233.\n",
      "Accuracy= 0.8506239918499023\n",
      "Precision= tensor(0.6914, device='cuda:0')\n",
      "Recall= tensor(0.7951, device='cuda:0')\n",
      "F1-Score= tensor(0.7396, device='cuda:0')\n",
      "At iteration 2000 the loss is 0.395.\n",
      "Accuracy= 0.8445750912641141\n",
      "Precision= tensor(0.6710, device='cuda:0')\n",
      "Recall= tensor(0.8192, device='cuda:0')\n",
      "F1-Score= tensor(0.7377, device='cuda:0')\n",
      "At iteration 2100 the loss is 0.414.\n",
      "Accuracy= 0.8494778843704899\n",
      "Precision= tensor(0.6867, device='cuda:0')\n",
      "Recall= tensor(0.8016, device='cuda:0')\n",
      "F1-Score= tensor(0.7397, device='cuda:0')\n",
      "At iteration 2200 the loss is 0.427.\n",
      "Accuracy= 0.8509848034637915\n",
      "Precision= tensor(0.6917, device='cuda:0')\n",
      "Recall= tensor(0.7965, device='cuda:0')\n",
      "F1-Score= tensor(0.7404, device='cuda:0')\n",
      "At iteration 2300 the loss is 0.414.\n",
      "Accuracy= 0.8502207318108498\n",
      "Precision= tensor(0.6888, device='cuda:0')\n",
      "Recall= tensor(0.8001, device='cuda:0')\n",
      "F1-Score= tensor(0.7403, device='cuda:0')\n",
      "At iteration 2400 the loss is 0.375.\n",
      "Accuracy= 0.8471219967739196\n",
      "Precision= tensor(0.6812, device='cuda:0')\n",
      "Recall= tensor(0.8027, device='cuda:0')\n",
      "F1-Score= tensor(0.7370, device='cuda:0')\n",
      "At iteration 2500 the loss is 0.235.\n",
      "Accuracy= 0.8510484761015367\n",
      "Precision= tensor(0.6900, device='cuda:0')\n",
      "Recall= tensor(0.8021, device='cuda:0')\n",
      "F1-Score= tensor(0.7419, device='cuda:0')\n",
      "At iteration 2600 the loss is 0.448.\n",
      "Accuracy= 0.8447873333899313\n",
      "Precision= tensor(0.6725, device='cuda:0')\n",
      "Recall= tensor(0.8153, device='cuda:0')\n",
      "F1-Score= tensor(0.7371, device='cuda:0')\n",
      "At iteration 2700 the loss is 0.461.\n",
      "Accuracy= 0.849095848544019\n",
      "Precision= tensor(0.6830, device='cuda:0')\n",
      "Recall= tensor(0.8106, device='cuda:0')\n",
      "F1-Score= tensor(0.7414, device='cuda:0')\n",
      "At iteration 2800 the loss is 0.310.\n",
      "Accuracy= 0.8487350369301299\n",
      "Precision= tensor(0.6807, device='cuda:0')\n",
      "Recall= tensor(0.8157, device='cuda:0')\n",
      "F1-Score= tensor(0.7421, device='cuda:0')\n",
      "At iteration 2900 the loss is 0.287.\n",
      "Accuracy= 0.8160285253417098\n",
      "Precision= tensor(0.6076, device='cuda:0')\n",
      "Recall= tensor(0.8767, device='cuda:0')\n",
      "F1-Score= tensor(0.7178, device='cuda:0')\n",
      "At iteration 100 the loss is 0.235.\n",
      "Accuracy= 0.8546141438152645\n",
      "Precision= tensor(0.6999, device='cuda:0')\n",
      "Recall= tensor(0.7968, device='cuda:0')\n",
      "F1-Score= tensor(0.7452, device='cuda:0')\n",
      "At iteration 200 the loss is 0.422.\n",
      "Accuracy= 0.8367009083962985\n",
      "Precision= tensor(0.6493, device='cuda:0')\n",
      "Recall= tensor(0.8436, device='cuda:0')\n",
      "F1-Score= tensor(0.7338, device='cuda:0')\n",
      "At iteration 300 the loss is 0.314.\n",
      "Accuracy= 0.8454028355548009\n",
      "Precision= tensor(0.6724, device='cuda:0')\n",
      "Recall= tensor(0.8202, device='cuda:0')\n",
      "F1-Score= tensor(0.7390, device='cuda:0')\n",
      "At iteration 400 the loss is 0.205.\n",
      "Accuracy= 0.8527676373206554\n",
      "Precision= tensor(0.6955, device='cuda:0')\n",
      "Recall= tensor(0.7972, device='cuda:0')\n",
      "F1-Score= tensor(0.7429, device='cuda:0')\n",
      "At iteration 500 the loss is 0.366.\n",
      "Accuracy= 0.8430681721708124\n",
      "Precision= tensor(0.6663, device='cuda:0')\n",
      "Recall= tensor(0.8252, device='cuda:0')\n",
      "F1-Score= tensor(0.7373, device='cuda:0')\n",
      "At iteration 600 the loss is 0.365.\n",
      "Accuracy= 0.8327107564309364\n",
      "Precision= tensor(0.6422, device='cuda:0')\n",
      "Recall= tensor(0.8422, device='cuda:0')\n",
      "F1-Score= tensor(0.7287, device='cuda:0')\n",
      "At iteration 700 the loss is 0.340.\n",
      "Accuracy= 0.8464852703964683\n",
      "Precision= tensor(0.6750, device='cuda:0')\n",
      "Recall= tensor(0.8191, device='cuda:0')\n",
      "F1-Score= tensor(0.7401, device='cuda:0')\n",
      "At iteration 800 the loss is 0.160.\n",
      "Accuracy= 0.8397784192206469\n",
      "Precision= tensor(0.6592, device='cuda:0')\n",
      "Recall= tensor(0.8273, device='cuda:0')\n",
      "F1-Score= tensor(0.7337, device='cuda:0')\n",
      "At iteration 900 the loss is 0.162.\n",
      "Accuracy= 0.8306944562356736\n",
      "Precision= tensor(0.6375, device='cuda:0')\n",
      "Recall= tensor(0.8475, device='cuda:0')\n",
      "F1-Score= tensor(0.7276, device='cuda:0')\n",
      "At iteration 1000 the loss is 0.326.\n",
      "Accuracy= 0.8357670430427031\n",
      "Precision= tensor(0.6480, device='cuda:0')\n",
      "Recall= tensor(0.8418, device='cuda:0')\n",
      "F1-Score= tensor(0.7323, device='cuda:0')\n",
      "At iteration 1100 the loss is 0.399.\n",
      "Accuracy= 0.8442355038628067\n",
      "Precision= tensor(0.6684, device='cuda:0')\n",
      "Recall= tensor(0.8259, device='cuda:0')\n",
      "F1-Score= tensor(0.7389, device='cuda:0')\n",
      "At iteration 1200 the loss is 0.474.\n",
      "Accuracy= 0.8316070973766874\n",
      "Precision= tensor(0.6404, device='cuda:0')\n",
      "Recall= tensor(0.8416, device='cuda:0')\n",
      "F1-Score= tensor(0.7273, device='cuda:0')\n",
      "At iteration 1300 the loss is 0.264.\n",
      "Accuracy= 0.8332838101706427\n",
      "Precision= tensor(0.6431, device='cuda:0')\n",
      "Recall= tensor(0.8431, device='cuda:0')\n",
      "F1-Score= tensor(0.7297, device='cuda:0')\n",
      "At iteration 1400 the loss is 0.404.\n",
      "Accuracy= 0.8341327786739112\n",
      "Precision= tensor(0.6449, device='cuda:0')\n",
      "Recall= tensor(0.8420, device='cuda:0')\n",
      "F1-Score= tensor(0.7304, device='cuda:0')\n",
      "At iteration 1500 the loss is 0.329.\n",
      "Accuracy= 0.8438534680363359\n",
      "Precision= tensor(0.6674, device='cuda:0')\n",
      "Recall= tensor(0.8270, device='cuda:0')\n",
      "F1-Score= tensor(0.7387, device='cuda:0')\n",
      "At iteration 1600 the loss is 0.380.\n",
      "Accuracy= 0.8388233296544698\n",
      "Precision= tensor(0.6562, device='cuda:0')\n",
      "Recall= tensor(0.8317, device='cuda:0')\n",
      "F1-Score= tensor(0.7336, device='cuda:0')\n",
      "At iteration 1700 the loss is 0.296.\n",
      "Accuracy= 0.8527888615332371\n",
      "Precision= tensor(0.6954, device='cuda:0')\n",
      "Recall= tensor(0.7976, device='cuda:0')\n",
      "F1-Score= tensor(0.7430, device='cuda:0')\n",
      "At iteration 1800 the loss is 0.425.\n",
      "Accuracy= 0.8478223957891162\n",
      "Precision= tensor(0.6787, device='cuda:0')\n",
      "Recall= tensor(0.8160, device='cuda:0')\n",
      "F1-Score= tensor(0.7410, device='cuda:0')\n",
      "At iteration 1900 the loss is 0.346.\n",
      "Accuracy= 0.8361915272943374\n",
      "Precision= tensor(0.6499, device='cuda:0')\n",
      "Recall= tensor(0.8371, device='cuda:0')\n",
      "F1-Score= tensor(0.7317, device='cuda:0')\n",
      "At iteration 2000 the loss is 0.308.\n",
      "Accuracy= 0.8467611851600305\n",
      "Precision= tensor(0.6762, device='cuda:0')\n",
      "Recall= tensor(0.8169, device='cuda:0')\n",
      "F1-Score= tensor(0.7399, device='cuda:0')\n",
      "At iteration 2100 the loss is 0.235.\n",
      "Accuracy= 0.845105696578657\n",
      "Precision= tensor(0.6724, device='cuda:0')\n",
      "Recall= tensor(0.8180, device='cuda:0')\n",
      "F1-Score= tensor(0.7381, device='cuda:0')\n",
      "At iteration 2200 the loss is 0.359.\n",
      "Accuracy= 0.8399694371338823\n",
      "Precision= tensor(0.6591, device='cuda:0')\n",
      "Recall= tensor(0.8290, device='cuda:0')\n",
      "F1-Score= tensor(0.7344, device='cuda:0')\n",
      "At iteration 2300 the loss is 0.354.\n",
      "Accuracy= 0.8383351727650904\n",
      "Precision= tensor(0.6561, device='cuda:0')\n",
      "Recall= tensor(0.8282, device='cuda:0')\n",
      "F1-Score= tensor(0.7322, device='cuda:0')\n",
      "At iteration 2400 the loss is 0.286.\n",
      "Accuracy= 0.8376347737498939\n",
      "Precision= tensor(0.6534, device='cuda:0')\n",
      "Recall= tensor(0.8340, device='cuda:0')\n",
      "F1-Score= tensor(0.7327, device='cuda:0')\n",
      "At iteration 2500 the loss is 0.283.\n",
      "Accuracy= 0.8400118855590457\n",
      "Precision= tensor(0.6576, device='cuda:0')\n",
      "Recall= tensor(0.8355, device='cuda:0')\n",
      "F1-Score= tensor(0.7359, device='cuda:0')\n",
      "At iteration 2600 the loss is 0.304.\n",
      "Accuracy= 0.8431742932337211\n",
      "Precision= tensor(0.6656, device='cuda:0')\n",
      "Recall= tensor(0.8285, device='cuda:0')\n",
      "F1-Score= tensor(0.7382, device='cuda:0')\n",
      "At iteration 2700 the loss is 0.421.\n",
      "Accuracy= 0.8363188725698276\n",
      "Precision= tensor(0.6488, device='cuda:0')\n",
      "Recall= tensor(0.8428, device='cuda:0')\n",
      "F1-Score= tensor(0.7332, device='cuda:0')\n",
      "At iteration 2800 the loss is 0.304.\n",
      "Accuracy= 0.8456575261057815\n",
      "Precision= tensor(0.6725, device='cuda:0')\n",
      "Recall= tensor(0.8218, device='cuda:0')\n",
      "F1-Score= tensor(0.7397, device='cuda:0')\n",
      "At iteration 2900 the loss is 0.227.\n",
      "Accuracy= 0.8507088887002292\n",
      "Precision= tensor(0.6848, device='cuda:0')\n",
      "Recall= tensor(0.8160, device='cuda:0')\n",
      "F1-Score= tensor(0.7447, device='cuda:0')\n",
      "At iteration 100 the loss is 0.243.\n",
      "Accuracy= 0.8483530011036591\n",
      "Precision= tensor(0.6800, device='cuda:0')\n",
      "Recall= tensor(0.8155, device='cuda:0')\n",
      "F1-Score= tensor(0.7416, device='cuda:0')\n",
      "At iteration 200 the loss is 0.402.\n",
      "Accuracy= 0.8402029034722812\n",
      "Precision= tensor(0.6606, device='cuda:0')\n",
      "Recall= tensor(0.8249, device='cuda:0')\n",
      "F1-Score= tensor(0.7337, device='cuda:0')\n",
      "At iteration 300 the loss is 0.189.\n",
      "Accuracy= 0.8320952542660667\n",
      "Precision= tensor(0.6426, device='cuda:0')\n",
      "Recall= tensor(0.8353, device='cuda:0')\n",
      "F1-Score= tensor(0.7264, device='cuda:0')\n",
      "At iteration 400 the loss is 0.143.\n",
      "Accuracy= 0.8323074963918838\n",
      "Precision= tensor(0.6430, device='cuda:0')\n",
      "Recall= tensor(0.8353, device='cuda:0')\n",
      "F1-Score= tensor(0.7266, device='cuda:0')\n",
      "At iteration 500 the loss is 0.324.\n",
      "Accuracy= 0.8322013753289753\n",
      "Precision= tensor(0.6453, device='cuda:0')\n",
      "Recall= tensor(0.8241, device='cuda:0')\n",
      "F1-Score= tensor(0.7238, device='cuda:0')\n",
      "At iteration 600 the loss is 0.192.\n",
      "Accuracy= 0.829633245606588\n",
      "Precision= tensor(0.6375, device='cuda:0')\n",
      "Recall= tensor(0.8381, device='cuda:0')\n",
      "F1-Score= tensor(0.7241, device='cuda:0')\n",
      "At iteration 700 the loss is 0.384.\n",
      "Accuracy= 0.8234994481704728\n",
      "Precision= tensor(0.6247, device='cuda:0')\n",
      "Recall= tensor(0.8478, device='cuda:0')\n",
      "F1-Score= tensor(0.7194, device='cuda:0')\n",
      "At iteration 800 the loss is 0.216.\n",
      "Accuracy= 0.8414975804397656\n",
      "Precision= tensor(0.6659, device='cuda:0')\n",
      "Recall= tensor(0.8147, device='cuda:0')\n",
      "F1-Score= tensor(0.7328, device='cuda:0')\n",
      "At iteration 900 the loss is 0.229.\n",
      "Accuracy= 0.8421130826046354\n",
      "Precision= tensor(0.6665, device='cuda:0')\n",
      "Recall= tensor(0.8171, device='cuda:0')\n",
      "F1-Score= tensor(0.7342, device='cuda:0')\n",
      "At iteration 1000 the loss is 0.250.\n",
      "Accuracy= 0.8252398336021733\n",
      "Precision= tensor(0.6269, device='cuda:0')\n",
      "Recall= tensor(0.8524, device='cuda:0')\n",
      "F1-Score= tensor(0.7224, device='cuda:0')\n",
      "At iteration 1100 the loss is 0.094.\n",
      "Accuracy= 0.8361278546565922\n",
      "Precision= tensor(0.6529, device='cuda:0')\n",
      "Recall= tensor(0.8239, device='cuda:0')\n",
      "F1-Score= tensor(0.7285, device='cuda:0')\n",
      "At iteration 1200 the loss is 0.104.\n",
      "Accuracy= 0.8553357670430427\n",
      "Precision= tensor(0.7081, device='cuda:0')\n",
      "Recall= tensor(0.7789, device='cuda:0')\n",
      "F1-Score= tensor(0.7418, device='cuda:0')\n",
      "At iteration 1300 the loss is 0.206.\n",
      "Accuracy= 0.8403090245351897\n",
      "Precision= tensor(0.6644, device='cuda:0')\n",
      "Recall= tensor(0.8112, device='cuda:0')\n",
      "F1-Score= tensor(0.7305, device='cuda:0')\n",
      "At iteration 1400 the loss is 0.249.\n",
      "Accuracy= 0.8269589948212921\n",
      "Precision= tensor(0.6314, device='cuda:0')\n",
      "Recall= tensor(0.8443, device='cuda:0')\n",
      "F1-Score= tensor(0.7225, device='cuda:0')\n",
      "At iteration 1500 the loss is 0.240.\n",
      "Accuracy= 0.8353425587910689\n",
      "Precision= tensor(0.6500, device='cuda:0')\n",
      "Recall= tensor(0.8297, device='cuda:0')\n",
      "F1-Score= tensor(0.7289, device='cuda:0')\n",
      "At iteration 1600 the loss is 0.204.\n",
      "Accuracy= 0.8315434247389422\n",
      "Precision= tensor(0.6417, device='cuda:0')\n",
      "Recall= tensor(0.8347, device='cuda:0')\n",
      "F1-Score= tensor(0.7256, device='cuda:0')\n",
      "At iteration 1700 the loss is 0.263.\n",
      "Accuracy= 0.8354699040665591\n",
      "Precision= tensor(0.6509, device='cuda:0')\n",
      "Recall= tensor(0.8268, device='cuda:0')\n",
      "F1-Score= tensor(0.7284, device='cuda:0')\n",
      "At iteration 1800 the loss is 0.253.\n",
      "Accuracy= 0.8398208676458103\n",
      "Precision= tensor(0.6612, device='cuda:0')\n",
      "Recall= tensor(0.8198, device='cuda:0')\n",
      "F1-Score= tensor(0.7320, device='cuda:0')\n",
      "At iteration 1900 the loss is 0.341.\n",
      "Accuracy= 0.8384200696154173\n",
      "Precision= tensor(0.6569, device='cuda:0')\n",
      "Recall= tensor(0.8256, device='cuda:0')\n",
      "F1-Score= tensor(0.7317, device='cuda:0')\n",
      "At iteration 2000 the loss is 0.175.\n",
      "Accuracy= 0.8317981152899228\n",
      "Precision= tensor(0.6419, device='cuda:0')\n",
      "Recall= tensor(0.8361, device='cuda:0')\n",
      "F1-Score= tensor(0.7262, device='cuda:0')\n",
      "At iteration 2100 the loss is 0.209.\n",
      "Accuracy= 0.838144154851855\n",
      "Precision= tensor(0.6562, device='cuda:0')\n",
      "Recall= tensor(0.8264, device='cuda:0')\n",
      "F1-Score= tensor(0.7315, device='cuda:0')\n",
      "At iteration 2200 the loss is 0.294.\n",
      "Accuracy= 0.8398208676458103\n",
      "Precision= tensor(0.6605, device='cuda:0')\n",
      "Recall= tensor(0.8223, device='cuda:0')\n",
      "F1-Score= tensor(0.7326, device='cuda:0')\n",
      "At iteration 2300 the loss is 0.230.\n",
      "Accuracy= 0.8394388318193395\n",
      "Precision= tensor(0.6591, device='cuda:0')\n",
      "Recall= tensor(0.8250, device='cuda:0')\n",
      "F1-Score= tensor(0.7328, device='cuda:0')\n",
      "At iteration 2400 the loss is 0.194.\n",
      "Accuracy= 0.8453391629170558\n",
      "Precision= tensor(0.6762, device='cuda:0')\n",
      "Recall= tensor(0.8065, device='cuda:0')\n",
      "F1-Score= tensor(0.7356, device='cuda:0')\n",
      "At iteration 2500 the loss is 0.316.\n",
      "Accuracy= 0.8445326428389507\n",
      "Precision= tensor(0.6735, device='cuda:0')\n",
      "Recall= tensor(0.8102, device='cuda:0')\n",
      "F1-Score= tensor(0.7355, device='cuda:0')\n",
      "At iteration 2600 the loss is 0.300.\n",
      "Accuracy= 0.8485864674420579\n",
      "Precision= tensor(0.6850, device='cuda:0')\n",
      "Recall= tensor(0.8007, device='cuda:0')\n",
      "F1-Score= tensor(0.7384, device='cuda:0')\n",
      "At iteration 2700 the loss is 0.342.\n",
      "Accuracy= 0.8338144154851855\n",
      "Precision= tensor(0.6451, device='cuda:0')\n",
      "Recall= tensor(0.8385, device='cuda:0')\n",
      "F1-Score= tensor(0.7292, device='cuda:0')\n",
      "At iteration 2800 the loss is 0.272.\n",
      "Accuracy= 0.83436624501231\n",
      "Precision= tensor(0.6460, device='cuda:0')\n",
      "Recall= tensor(0.8392, device='cuda:0')\n",
      "F1-Score= tensor(0.7300, device='cuda:0')\n",
      "At iteration 2900 the loss is 0.348.\n",
      "Accuracy= 0.8382502759147635\n",
      "Precision= tensor(0.6546, device='cuda:0')\n",
      "Recall= tensor(0.8336, device='cuda:0')\n",
      "F1-Score= tensor(0.7334, device='cuda:0')\n",
      "At iteration 100 the loss is 0.296.\n",
      "Accuracy= 0.8291663129297903\n",
      "Precision= tensor(0.6366, device='cuda:0')\n",
      "Recall= tensor(0.8384, device='cuda:0')\n",
      "F1-Score= tensor(0.7237, device='cuda:0')\n",
      "At iteration 200 the loss is 0.160.\n",
      "Accuracy= 0.8380168095763647\n",
      "Precision= tensor(0.6585, device='cuda:0')\n",
      "Recall= tensor(0.8163, device='cuda:0')\n",
      "F1-Score= tensor(0.7289, device='cuda:0')\n",
      "At iteration 300 the loss is 0.243.\n",
      "Accuracy= 0.8441506070124798\n",
      "Precision= tensor(0.6768, device='cuda:0')\n",
      "Recall= tensor(0.7961, device='cuda:0')\n",
      "F1-Score= tensor(0.7316, device='cuda:0')\n",
      "At iteration 400 the loss is 0.210.\n",
      "Accuracy= 0.8431318448085576\n",
      "Precision= tensor(0.6744, device='cuda:0')\n",
      "Recall= tensor(0.7969, device='cuda:0')\n",
      "F1-Score= tensor(0.7305, device='cuda:0')\n",
      "At iteration 500 the loss is 0.187.\n",
      "Accuracy= 0.8444052975634604\n",
      "Precision= tensor(0.6792, device='cuda:0')\n",
      "Recall= tensor(0.7899, device='cuda:0')\n",
      "F1-Score= tensor(0.7304, device='cuda:0')\n",
      "At iteration 600 the loss is 0.105.\n",
      "Accuracy= 0.838144154851855\n",
      "Precision= tensor(0.6601, device='cuda:0')\n",
      "Recall= tensor(0.8110, device='cuda:0')\n",
      "F1-Score= tensor(0.7278, device='cuda:0')\n",
      "At iteration 700 the loss is 0.234.\n",
      "Accuracy= 0.8461032345699975\n",
      "Precision= tensor(0.6856, device='cuda:0')\n",
      "Recall= tensor(0.7818, device='cuda:0')\n",
      "F1-Score= tensor(0.7305, device='cuda:0')\n",
      "At iteration 800 the loss is 0.284.\n",
      "Accuracy= 0.8310764920621445\n",
      "Precision= tensor(0.6436, device='cuda:0')\n",
      "Recall= tensor(0.8221, device='cuda:0')\n",
      "F1-Score= tensor(0.7220, device='cuda:0')\n",
      "At iteration 900 the loss is 0.170.\n",
      "Accuracy= 0.8340903302487478\n",
      "Precision= tensor(0.6516, device='cuda:0')\n",
      "Recall= tensor(0.8128, device='cuda:0')\n",
      "F1-Score= tensor(0.7233, device='cuda:0')\n",
      "At iteration 1000 the loss is 0.150.\n",
      "Accuracy= 0.8419645131165634\n",
      "Precision= tensor(0.6724, device='cuda:0')\n",
      "Recall= tensor(0.7950, device='cuda:0')\n",
      "F1-Score= tensor(0.7286, device='cuda:0')\n",
      "At iteration 1100 the loss is 0.152.\n",
      "Accuracy= 0.8178750318363188\n",
      "Precision= tensor(0.6166, device='cuda:0')\n",
      "Recall= tensor(0.8395, device='cuda:0')\n",
      "F1-Score= tensor(0.7110, device='cuda:0')\n",
      "At iteration 1200 the loss is 0.250.\n",
      "Accuracy= 0.8319891332031581\n",
      "Precision= tensor(0.6470, device='cuda:0')\n",
      "Recall= tensor(0.8149, device='cuda:0')\n",
      "F1-Score= tensor(0.7213, device='cuda:0')\n",
      "At iteration 1300 the loss is 0.206.\n",
      "Accuracy= 0.8365098904830631\n",
      "Precision= tensor(0.6567, device='cuda:0')\n",
      "Recall= tensor(0.8114, device='cuda:0')\n",
      "F1-Score= tensor(0.7259, device='cuda:0')\n",
      "At iteration 1400 the loss is 0.080.\n",
      "Accuracy= 0.8244333135240682\n",
      "Precision= tensor(0.6287, device='cuda:0')\n",
      "Recall= tensor(0.8353, device='cuda:0')\n",
      "F1-Score= tensor(0.7174, device='cuda:0')\n",
      "At iteration 1500 the loss is 0.203.\n",
      "Accuracy= 0.8218864080142627\n",
      "Precision= tensor(0.6249, device='cuda:0')\n",
      "Recall= tensor(0.8318, device='cuda:0')\n",
      "F1-Score= tensor(0.7136, device='cuda:0')\n",
      "At iteration 1600 the loss is 0.188.\n",
      "Accuracy= 0.8322650479667204\n",
      "Precision= tensor(0.6454, device='cuda:0')\n",
      "Recall= tensor(0.8241, device='cuda:0')\n",
      "F1-Score= tensor(0.7239, device='cuda:0')\n",
      "At iteration 1700 the loss is 0.223.\n",
      "Accuracy= 0.8321589269038119\n",
      "Precision= tensor(0.6457, device='cuda:0')\n",
      "Recall= tensor(0.8218, device='cuda:0')\n",
      "F1-Score= tensor(0.7232, device='cuda:0')\n",
      "At iteration 1800 the loss is 0.308.\n",
      "Accuracy= 0.8273834790729264\n",
      "Precision= tensor(0.6341, device='cuda:0')\n",
      "Recall= tensor(0.8348, device='cuda:0')\n",
      "F1-Score= tensor(0.7207, device='cuda:0')\n",
      "At iteration 1900 the loss is 0.124.\n",
      "Accuracy= 0.8444052975634604\n",
      "Precision= tensor(0.6769, device='cuda:0')\n",
      "Recall= tensor(0.7976, device='cuda:0')\n",
      "F1-Score= tensor(0.7323, device='cuda:0')\n",
      "At iteration 2000 the loss is 0.274.\n",
      "Accuracy= 0.8214619237626284\n",
      "Precision= tensor(0.6222, device='cuda:0')\n",
      "Recall= tensor(0.8424, device='cuda:0')\n",
      "F1-Score= tensor(0.7158, device='cuda:0')\n",
      "At iteration 2100 the loss is 0.141.\n",
      "Accuracy= 0.8272349095848543\n",
      "Precision= tensor(0.6341, device='cuda:0')\n",
      "Recall= tensor(0.8334, device='cuda:0')\n",
      "F1-Score= tensor(0.7202, device='cuda:0')\n",
      "At iteration 2200 the loss is 0.228.\n",
      "Accuracy= 0.8354699040665591\n",
      "Precision= tensor(0.6543, device='cuda:0')\n",
      "Recall= tensor(0.8127, device='cuda:0')\n",
      "F1-Score= tensor(0.7250, device='cuda:0')\n",
      "At iteration 2300 the loss is 0.329.\n",
      "Accuracy= 0.8217378385261906\n",
      "Precision= tensor(0.6238, device='cuda:0')\n",
      "Recall= tensor(0.8361, device='cuda:0')\n",
      "F1-Score= tensor(0.7145, device='cuda:0')\n",
      "At iteration 2400 the loss is 0.176.\n",
      "Accuracy= 0.8283810170642669\n",
      "Precision= tensor(0.6375, device='cuda:0')\n",
      "Recall= tensor(0.8271, device='cuda:0')\n",
      "F1-Score= tensor(0.7200, device='cuda:0')\n",
      "At iteration 2500 the loss is 0.173.\n",
      "Accuracy= 0.8430681721708124\n",
      "Precision= tensor(0.6747, device='cuda:0')\n",
      "Recall= tensor(0.7954, device='cuda:0')\n",
      "F1-Score= tensor(0.7301, device='cuda:0')\n",
      "At iteration 2600 the loss is 0.255.\n",
      "Accuracy= 0.8257916631292979\n",
      "Precision= tensor(0.6319, device='cuda:0')\n",
      "Recall= tensor(0.8315, device='cuda:0')\n",
      "F1-Score= tensor(0.7181, device='cuda:0')\n",
      "At iteration 2700 the loss is 0.191.\n",
      "Accuracy= 0.8378682400882927\n",
      "Precision= tensor(0.6606, device='cuda:0')\n",
      "Recall= tensor(0.8070, device='cuda:0')\n",
      "F1-Score= tensor(0.7265, device='cuda:0')\n",
      "At iteration 2800 the loss is 0.180.\n",
      "Accuracy= 0.8387172085915613\n",
      "Precision= tensor(0.6621, device='cuda:0')\n",
      "Recall= tensor(0.8077, device='cuda:0')\n",
      "F1-Score= tensor(0.7277, device='cuda:0')\n",
      "At iteration 2900 the loss is 0.213.\n",
      "Accuracy= 0.8242635198234145\n",
      "Precision= tensor(0.6284, device='cuda:0')\n",
      "Recall= tensor(0.8352, device='cuda:0')\n",
      "F1-Score= tensor(0.7172, device='cuda:0')\n",
      "At iteration 100 the loss is 0.183.\n",
      "Accuracy= 0.8432167416588845\n",
      "Precision= tensor(0.6759, device='cuda:0')\n",
      "Recall= tensor(0.7924, device='cuda:0')\n",
      "F1-Score= tensor(0.7295, device='cuda:0')\n",
      "At iteration 200 the loss is 0.116.\n",
      "Accuracy= 0.8446175396892776\n",
      "Precision= tensor(0.6855, device='cuda:0')\n",
      "Recall= tensor(0.7718, device='cuda:0')\n",
      "F1-Score= tensor(0.7261, device='cuda:0')\n",
      "At iteration 300 the loss is 0.246.\n",
      "Accuracy= 0.8378045674505475\n",
      "Precision= tensor(0.6647, device='cuda:0')\n",
      "Recall= tensor(0.7914, device='cuda:0')\n",
      "F1-Score= tensor(0.7225, device='cuda:0')\n",
      "At iteration 400 the loss is 0.159.\n",
      "Accuracy= 0.8359156125307751\n",
      "Precision= tensor(0.6602, device='cuda:0')\n",
      "Recall= tensor(0.7933, device='cuda:0')\n",
      "F1-Score= tensor(0.7207, device='cuda:0')\n",
      "At iteration 500 the loss is 0.126.\n",
      "Accuracy= 0.8459758892945072\n",
      "Precision= tensor(0.6891, device='cuda:0')\n",
      "Recall= tensor(0.7704, device='cuda:0')\n",
      "F1-Score= tensor(0.7275, device='cuda:0')\n",
      "At iteration 600 the loss is 0.113.\n",
      "Accuracy= 0.8266406316325664\n",
      "Precision= tensor(0.6368, device='cuda:0')\n",
      "Recall= tensor(0.8153, device='cuda:0')\n",
      "F1-Score= tensor(0.7151, device='cuda:0')\n",
      "At iteration 700 the loss is 0.419.\n",
      "Accuracy= 0.8370404957976059\n",
      "Precision= tensor(0.6641, device='cuda:0')\n",
      "Recall= tensor(0.7876, device='cuda:0')\n",
      "F1-Score= tensor(0.7206, device='cuda:0')\n",
      "At iteration 800 the loss is 0.165.\n",
      "Accuracy= 0.8374649800492402\n",
      "Precision= tensor(0.6644, device='cuda:0')\n",
      "Recall= tensor(0.7898, device='cuda:0')\n",
      "F1-Score= tensor(0.7217, device='cuda:0')\n",
      "At iteration 900 the loss is 0.220.\n",
      "Accuracy= 0.848650140079803\n",
      "Precision= tensor(0.7025, device='cuda:0')\n",
      "Recall= tensor(0.7506, device='cuda:0')\n",
      "F1-Score= tensor(0.7258, device='cuda:0')\n",
      "At iteration 1000 the loss is 0.136.\n",
      "Accuracy= 0.8261736989557688\n",
      "Precision= tensor(0.6354, device='cuda:0')\n",
      "Recall= tensor(0.8179, device='cuda:0')\n",
      "F1-Score= tensor(0.7152, device='cuda:0')\n",
      "At iteration 1100 the loss is 0.093.\n",
      "Accuracy= 0.8373376347737499\n",
      "Precision= tensor(0.6647, device='cuda:0')\n",
      "Recall= tensor(0.7876, device='cuda:0')\n",
      "F1-Score= tensor(0.7210, device='cuda:0')\n",
      "At iteration 1200 the loss is 0.094.\n",
      "Accuracy= 0.8443840733508787\n",
      "Precision= tensor(0.6885, device='cuda:0')\n",
      "Recall= tensor(0.7611, device='cuda:0')\n",
      "F1-Score= tensor(0.7230, device='cuda:0')\n",
      "At iteration 1300 the loss is 0.169.\n",
      "Accuracy= 0.8497537991340521\n",
      "Precision= tensor(0.7128, device='cuda:0')\n",
      "Recall= tensor(0.7317, device='cuda:0')\n",
      "F1-Score= tensor(0.7221, device='cuda:0')\n",
      "At iteration 1400 the loss is 0.202.\n",
      "Accuracy= 0.824603107224722\n",
      "Precision= tensor(0.6329, device='cuda:0')\n",
      "Recall= tensor(0.8158, device='cuda:0')\n",
      "F1-Score= tensor(0.7128, device='cuda:0')\n",
      "At iteration 1500 the loss is 0.086.\n",
      "Accuracy= 0.8363825452075728\n",
      "Precision= tensor(0.6614, device='cuda:0')\n",
      "Recall= tensor(0.7926, device='cuda:0')\n",
      "F1-Score= tensor(0.7211, device='cuda:0')\n",
      "At iteration 1600 the loss is 0.296.\n",
      "Accuracy= 0.8338356396977672\n",
      "Precision= tensor(0.6574, device='cuda:0')\n",
      "Recall= tensor(0.7878, device='cuda:0')\n",
      "F1-Score= tensor(0.7167, device='cuda:0')\n",
      "At iteration 1700 the loss is 0.214.\n",
      "Accuracy= 0.8315646489515239\n",
      "Precision= tensor(0.6506, device='cuda:0')\n",
      "Recall= tensor(0.7966, device='cuda:0')\n",
      "F1-Score= tensor(0.7162, device='cuda:0')\n",
      "At iteration 1800 the loss is 0.288.\n",
      "Accuracy= 0.8363188725698276\n",
      "Precision= tensor(0.6663, device='cuda:0')\n",
      "Recall= tensor(0.7743, device='cuda:0')\n",
      "F1-Score= tensor(0.7163, device='cuda:0')\n",
      "At iteration 1900 the loss is 0.183.\n",
      "Accuracy= 0.8244757619492317\n",
      "Precision= tensor(0.6336, device='cuda:0')\n",
      "Recall= tensor(0.8116, device='cuda:0')\n",
      "F1-Score= tensor(0.7116, device='cuda:0')\n",
      "At iteration 2000 the loss is 0.119.\n",
      "Accuracy= 0.8350454198149249\n",
      "Precision= tensor(0.6575, device='cuda:0')\n",
      "Recall= tensor(0.7969, device='cuda:0')\n",
      "F1-Score= tensor(0.7205, device='cuda:0')\n",
      "At iteration 2100 the loss is 0.318.\n",
      "Accuracy= 0.8373800831989133\n",
      "Precision= tensor(0.6637, device='cuda:0')\n",
      "Recall= tensor(0.7918, device='cuda:0')\n",
      "F1-Score= tensor(0.7221, device='cuda:0')\n",
      "At iteration 2200 the loss is 0.172.\n",
      "Accuracy= 0.8342388997368198\n",
      "Precision= tensor(0.6548, device='cuda:0')\n",
      "Recall= tensor(0.8013, device='cuda:0')\n",
      "F1-Score= tensor(0.7207, device='cuda:0')\n",
      "At iteration 2300 the loss is 0.157.\n",
      "Accuracy= 0.8331989133203158\n",
      "Precision= tensor(0.6517, device='cuda:0')\n",
      "Recall= tensor(0.8051, device='cuda:0')\n",
      "F1-Score= tensor(0.7204, device='cuda:0')\n",
      "At iteration 2400 the loss is 0.213.\n",
      "Accuracy= 0.8342176755242381\n",
      "Precision= tensor(0.6537, device='cuda:0')\n",
      "Recall= tensor(0.8051, device='cuda:0')\n",
      "F1-Score= tensor(0.7216, device='cuda:0')\n",
      "At iteration 2500 the loss is 0.208.\n",
      "Accuracy= 0.829633245606588\n",
      "Precision= tensor(0.6438, device='cuda:0')\n",
      "Recall= tensor(0.8093, device='cuda:0')\n",
      "F1-Score= tensor(0.7171, device='cuda:0')\n",
      "At iteration 2600 the loss is 0.081.\n",
      "Accuracy= 0.827192461159691\n",
      "Precision= tensor(0.6376, device='cuda:0')\n",
      "Recall= tensor(0.8163, device='cuda:0')\n",
      "F1-Score= tensor(0.7160, device='cuda:0')\n",
      "At iteration 2700 the loss is 0.126.\n",
      "Accuracy= 0.8298242635198234\n",
      "Precision= tensor(0.6453, device='cuda:0')\n",
      "Recall= tensor(0.8044, device='cuda:0')\n",
      "F1-Score= tensor(0.7161, device='cuda:0')\n",
      "At iteration 2800 the loss is 0.147.\n",
      "Accuracy= 0.8315858731641056\n",
      "Precision= tensor(0.6482, device='cuda:0')\n",
      "Recall= tensor(0.8066, device='cuda:0')\n",
      "F1-Score= tensor(0.7188, device='cuda:0')\n",
      "At iteration 2900 the loss is 0.293.\n",
      "Accuracy= 0.8362551999320825\n",
      "Precision= tensor(0.6634, device='cuda:0')\n",
      "Recall= tensor(0.7841, device='cuda:0')\n",
      "F1-Score= tensor(0.7187, device='cuda:0')\n",
      "At iteration 100 the loss is 0.074.\n",
      "Accuracy= 0.8377408948128025\n",
      "Precision= tensor(0.6711, device='cuda:0')\n",
      "Recall= tensor(0.7686, device='cuda:0')\n",
      "F1-Score= tensor(0.7165, device='cuda:0')\n",
      "At iteration 200 the loss is 0.035.\n",
      "Accuracy= 0.828890398166228\n",
      "Precision= tensor(0.6472, device='cuda:0')\n",
      "Recall= tensor(0.7888, device='cuda:0')\n",
      "F1-Score= tensor(0.7110, device='cuda:0')\n",
      "At iteration 300 the loss is 0.140.\n",
      "Accuracy= 0.8337507428474403\n",
      "Precision= tensor(0.6584, device='cuda:0')\n",
      "Recall= tensor(0.7836, device='cuda:0')\n",
      "F1-Score= tensor(0.7155, device='cuda:0')\n",
      "At iteration 400 the loss is 0.181.\n",
      "Accuracy= 0.8420918583920537\n",
      "Precision= tensor(0.6888, device='cuda:0')\n",
      "Recall= tensor(0.7445, device='cuda:0')\n",
      "F1-Score= tensor(0.7156, device='cuda:0')\n",
      "At iteration 500 the loss is 0.084.\n",
      "Accuracy= 0.8414551320146022\n",
      "Precision= tensor(0.6846, device='cuda:0')\n",
      "Recall= tensor(0.7525, device='cuda:0')\n",
      "F1-Score= tensor(0.7169, device='cuda:0')\n",
      "At iteration 600 the loss is 0.212.\n",
      "Accuracy= 0.8381866032770184\n",
      "Precision= tensor(0.6739, device='cuda:0')\n",
      "Recall= tensor(0.7626, device='cuda:0')\n",
      "F1-Score= tensor(0.7155, device='cuda:0')\n",
      "At iteration 700 the loss is 0.239.\n",
      "Accuracy= 0.8395237286696664\n",
      "Precision= tensor(0.6762, device='cuda:0')\n",
      "Recall= tensor(0.7649, device='cuda:0')\n",
      "F1-Score= tensor(0.7178, device='cuda:0')\n",
      "At iteration 800 the loss is 0.057.\n",
      "Accuracy= 0.8368707020969522\n",
      "Precision= tensor(0.6688, device='cuda:0')\n",
      "Recall= tensor(0.7698, device='cuda:0')\n",
      "F1-Score= tensor(0.7158, device='cuda:0')\n",
      "At iteration 900 the loss is 0.195.\n",
      "Accuracy= 0.8357882672552849\n",
      "Precision= tensor(0.6660, device='cuda:0')\n",
      "Recall= tensor(0.7714, device='cuda:0')\n",
      "F1-Score= tensor(0.7148, device='cuda:0')\n",
      "At iteration 1000 the loss is 0.163.\n",
      "Accuracy= 0.8409669751252229\n",
      "Precision= tensor(0.6885, device='cuda:0')\n",
      "Recall= tensor(0.7379, device='cuda:0')\n",
      "F1-Score= tensor(0.7123, device='cuda:0')\n",
      "At iteration 1100 the loss is 0.141.\n",
      "Accuracy= 0.8306944562356736\n",
      "Precision= tensor(0.6528, device='cuda:0')\n",
      "Recall= tensor(0.7808, device='cuda:0')\n",
      "F1-Score= tensor(0.7111, device='cuda:0')\n",
      "At iteration 1200 the loss is 0.128.\n",
      "Accuracy= 0.8322650479667204\n",
      "Precision= tensor(0.6551, device='cuda:0')\n",
      "Recall= tensor(0.7844, device='cuda:0')\n",
      "F1-Score= tensor(0.7139, device='cuda:0')\n",
      "At iteration 1300 the loss is 0.108.\n",
      "Accuracy= 0.8313524068257068\n",
      "Precision= tensor(0.6533, device='cuda:0')\n",
      "Recall= tensor(0.7840, device='cuda:0')\n",
      "F1-Score= tensor(0.7127, device='cuda:0')\n",
      "At iteration 1400 the loss is 0.121.\n",
      "Accuracy= 0.8308642499363273\n",
      "Precision= tensor(0.6534, device='cuda:0')\n",
      "Recall= tensor(0.7797, device='cuda:0')\n",
      "F1-Score= tensor(0.7110, device='cuda:0')\n",
      "At iteration 1500 the loss is 0.064.\n",
      "Accuracy= 0.8323287206044656\n",
      "Precision= tensor(0.6558, device='cuda:0')\n",
      "Recall= tensor(0.7823, device='cuda:0')\n",
      "F1-Score= tensor(0.7135, device='cuda:0')\n",
      "At iteration 1600 the loss is 0.203.\n",
      "Accuracy= 0.8162832158926904\n",
      "Precision= tensor(0.6175, device='cuda:0')\n",
      "Recall= tensor(0.8186, device='cuda:0')\n",
      "F1-Score= tensor(0.7039, device='cuda:0')\n",
      "At iteration 1700 the loss is 0.118.\n",
      "Accuracy= 0.8246880040750488\n",
      "Precision= tensor(0.6367, device='cuda:0')\n",
      "Recall= tensor(0.7988, device='cuda:0')\n",
      "F1-Score= tensor(0.7086, device='cuda:0')\n",
      "At iteration 1800 the loss is 0.168.\n",
      "Accuracy= 0.828890398166228\n",
      "Precision= tensor(0.6463, device='cuda:0')\n",
      "Recall= tensor(0.7923, device='cuda:0')\n",
      "F1-Score= tensor(0.7119, device='cuda:0')\n",
      "At iteration 1900 the loss is 0.175.\n",
      "Accuracy= 0.8191060361660583\n",
      "Precision= tensor(0.6252, device='cuda:0')\n",
      "Recall= tensor(0.8040, device='cuda:0')\n",
      "F1-Score= tensor(0.7034, device='cuda:0')\n",
      "At iteration 2000 the loss is 0.270.\n",
      "Accuracy= 0.8290177434417183\n",
      "Precision= tensor(0.6487, device='cuda:0')\n",
      "Recall= tensor(0.7836, device='cuda:0')\n",
      "F1-Score= tensor(0.7098, device='cuda:0')\n",
      "At iteration 2100 the loss is 0.190.\n",
      "Accuracy= 0.8393327107564309\n",
      "Precision= tensor(0.6794, device='cuda:0')\n",
      "Recall= tensor(0.7533, device='cuda:0')\n",
      "F1-Score= tensor(0.7145, device='cuda:0')\n",
      "At iteration 2200 the loss is 0.149.\n",
      "Accuracy= 0.8206554036845233\n",
      "Precision= tensor(0.6287, device='cuda:0')\n",
      "Recall= tensor(0.8008, device='cuda:0')\n",
      "F1-Score= tensor(0.7044, device='cuda:0')\n",
      "At iteration 2300 the loss is 0.153.\n",
      "Accuracy= 0.8340266576110026\n",
      "Precision= tensor(0.6614, device='cuda:0')\n",
      "Recall= tensor(0.7745, device='cuda:0')\n",
      "F1-Score= tensor(0.7135, device='cuda:0')\n",
      "At iteration 2400 the loss is 0.154.\n",
      "Accuracy= 0.8355335767043043\n",
      "Precision= tensor(0.6666, device='cuda:0')\n",
      "Recall= tensor(0.7673, device='cuda:0')\n",
      "F1-Score= tensor(0.7135, device='cuda:0')\n",
      "At iteration 2500 the loss is 0.066.\n",
      "Accuracy= 0.8299728330078954\n",
      "Precision= tensor(0.6510, device='cuda:0')\n",
      "Recall= tensor(0.7822, device='cuda:0')\n",
      "F1-Score= tensor(0.7106, device='cuda:0')\n",
      "At iteration 2600 the loss is 0.128.\n",
      "Accuracy= 0.82445453773665\n",
      "Precision= tensor(0.6370, device='cuda:0')\n",
      "Recall= tensor(0.7953, device='cuda:0')\n",
      "F1-Score= tensor(0.7074, device='cuda:0')\n",
      "At iteration 2700 the loss is 0.200.\n",
      "Accuracy= 0.8213133542745564\n",
      "Precision= tensor(0.6313, device='cuda:0')\n",
      "Recall= tensor(0.7939, device='cuda:0')\n",
      "F1-Score= tensor(0.7034, device='cuda:0')\n",
      "At iteration 2800 the loss is 0.388.\n",
      "Accuracy= 0.821270905849393\n",
      "Precision= tensor(0.6295, device='cuda:0')\n",
      "Recall= tensor(0.8027, device='cuda:0')\n",
      "F1-Score= tensor(0.7056, device='cuda:0')\n",
      "At iteration 2900 the loss is 0.138.\n",
      "Accuracy= 0.827786739111979\n",
      "Precision= tensor(0.6436, device='cuda:0')\n",
      "Recall= tensor(0.7945, device='cuda:0')\n",
      "F1-Score= tensor(0.7112, device='cuda:0')\n",
      "At iteration 100 the loss is 0.122.\n",
      "Accuracy= 0.8347695050513626\n",
      "Precision= tensor(0.6688, device='cuda:0')\n",
      "Recall= tensor(0.7542, device='cuda:0')\n",
      "F1-Score= tensor(0.7090, device='cuda:0')\n",
      "At iteration 200 the loss is 0.060.\n",
      "Accuracy= 0.8229688428559301\n",
      "Precision= tensor(0.6374, device='cuda:0')\n",
      "Recall= tensor(0.7806, device='cuda:0')\n",
      "F1-Score= tensor(0.7018, device='cuda:0')\n",
      "At iteration 300 the loss is 0.066.\n",
      "Accuracy= 0.8345784871381272\n",
      "Precision= tensor(0.6708, device='cuda:0')\n",
      "Recall= tensor(0.7463, device='cuda:0')\n",
      "F1-Score= tensor(0.7065, device='cuda:0')\n",
      "At iteration 400 the loss is 0.050.\n",
      "Accuracy= 0.8351727650904152\n",
      "Precision= tensor(0.6707, device='cuda:0')\n",
      "Recall= tensor(0.7511, device='cuda:0')\n",
      "F1-Score= tensor(0.7086, device='cuda:0')\n",
      "At iteration 500 the loss is 0.108.\n",
      "Accuracy= 0.8185117582137703\n",
      "Precision= tensor(0.6267, device='cuda:0')\n",
      "Recall= tensor(0.7910, device='cuda:0')\n",
      "F1-Score= tensor(0.6993, device='cuda:0')\n",
      "At iteration 600 the loss is 0.080.\n",
      "Accuracy= 0.82978181509466\n",
      "Precision= tensor(0.6531, device='cuda:0')\n",
      "Recall= tensor(0.7724, device='cuda:0')\n",
      "F1-Score= tensor(0.7077, device='cuda:0')\n",
      "At iteration 700 the loss is 0.156.\n",
      "Accuracy= 0.8341540028864929\n",
      "Precision= tensor(0.6662, device='cuda:0')\n",
      "Recall= tensor(0.7584, device='cuda:0')\n",
      "F1-Score= tensor(0.7093, device='cuda:0')\n",
      "At iteration 800 the loss is 0.117.\n",
      "Accuracy= 0.8372315137108414\n",
      "Precision= tensor(0.6767, device='cuda:0')\n",
      "Recall= tensor(0.7468, device='cuda:0')\n",
      "F1-Score= tensor(0.7100, device='cuda:0')\n",
      "At iteration 900 the loss is 0.195.\n",
      "Accuracy= 0.8285508107649207\n",
      "Precision= tensor(0.6511, device='cuda:0')\n",
      "Recall= tensor(0.7700, device='cuda:0')\n",
      "F1-Score= tensor(0.7056, device='cuda:0')\n",
      "At iteration 1000 the loss is 0.039.\n",
      "Accuracy= 0.8291450887172086\n",
      "Precision= tensor(0.6546, device='cuda:0')\n",
      "Recall= tensor(0.7616, device='cuda:0')\n",
      "F1-Score= tensor(0.7040, device='cuda:0')\n",
      "At iteration 1100 the loss is 0.240.\n",
      "Accuracy= 0.8343450207997283\n",
      "Precision= tensor(0.6686, device='cuda:0')\n",
      "Recall= tensor(0.7520, device='cuda:0')\n",
      "F1-Score= tensor(0.7078, device='cuda:0')\n",
      "At iteration 1200 the loss is 0.086.\n",
      "Accuracy= 0.8288691739536463\n",
      "Precision= tensor(0.6521, device='cuda:0')\n",
      "Recall= tensor(0.7688, device='cuda:0')\n",
      "F1-Score= tensor(0.7057, device='cuda:0')\n",
      "At iteration 1300 the loss is 0.055.\n",
      "Accuracy= 0.8358731641056116\n",
      "Precision= tensor(0.6753, device='cuda:0')\n",
      "Recall= tensor(0.7413, device='cuda:0')\n",
      "F1-Score= tensor(0.7068, device='cuda:0')\n",
      "At iteration 1400 the loss is 0.082.\n",
      "Accuracy= 0.8283385686391035\n",
      "Precision= tensor(0.6512, device='cuda:0')\n",
      "Recall= tensor(0.7681, device='cuda:0')\n",
      "F1-Score= tensor(0.7048, device='cuda:0')\n",
      "At iteration 1500 the loss is 0.031.\n",
      "Accuracy= 0.8390143475677052\n",
      "Precision= tensor(0.6864, device='cuda:0')\n",
      "Recall= tensor(0.7303, device='cuda:0')\n",
      "F1-Score= tensor(0.7077, device='cuda:0')\n",
      "At iteration 1600 the loss is 0.064.\n",
      "Accuracy= 0.8243908650989048\n",
      "Precision= tensor(0.6418, device='cuda:0')\n",
      "Recall= tensor(0.7739, device='cuda:0')\n",
      "F1-Score= tensor(0.7016, device='cuda:0')\n",
      "At iteration 1700 the loss is 0.122.\n",
      "Accuracy= 0.8303124204092028\n",
      "Precision= tensor(0.6576, device='cuda:0')\n",
      "Recall= tensor(0.7597, device='cuda:0')\n",
      "F1-Score= tensor(0.7049, device='cuda:0')\n",
      "At iteration 1800 the loss is 0.102.\n",
      "Accuracy= 0.824008829272434\n",
      "Precision= tensor(0.6394, device='cuda:0')\n",
      "Recall= tensor(0.7806, device='cuda:0')\n",
      "F1-Score= tensor(0.7030, device='cuda:0')\n",
      "At iteration 1900 the loss is 0.037.\n",
      "Accuracy= 0.8299516087953137\n",
      "Precision= tensor(0.6544, device='cuda:0')\n",
      "Recall= tensor(0.7688, device='cuda:0')\n",
      "F1-Score= tensor(0.7070, device='cuda:0')\n",
      "At iteration 2000 the loss is 0.136.\n",
      "Accuracy= 0.8383563969776722\n",
      "Precision= tensor(0.6814, device='cuda:0')\n",
      "Recall= tensor(0.7404, device='cuda:0')\n",
      "F1-Score= tensor(0.7097, device='cuda:0')\n",
      "At iteration 2100 the loss is 0.157.\n",
      "Accuracy= 0.8265345105696579\n",
      "Precision= tensor(0.6457, device='cuda:0')\n",
      "Recall= tensor(0.7755, device='cuda:0')\n",
      "F1-Score= tensor(0.7047, device='cuda:0')\n",
      "At iteration 2200 the loss is 0.052.\n",
      "Accuracy= 0.8271287885219458\n",
      "Precision= tensor(0.6486, device='cuda:0')\n",
      "Recall= tensor(0.7685, device='cuda:0')\n",
      "F1-Score= tensor(0.7035, device='cuda:0')\n",
      "At iteration 2300 the loss is 0.398.\n",
      "Accuracy= 0.8309279225740726\n",
      "Precision= tensor(0.6577, device='cuda:0')\n",
      "Recall= tensor(0.7639, device='cuda:0')\n",
      "F1-Score= tensor(0.7069, device='cuda:0')\n",
      "At iteration 2400 the loss is 0.022.\n",
      "Accuracy= 0.8242210713982511\n",
      "Precision= tensor(0.6385, device='cuda:0')\n",
      "Recall= tensor(0.7867, device='cuda:0')\n",
      "F1-Score= tensor(0.7049, device='cuda:0')\n",
      "At iteration 2500 the loss is 0.304.\n",
      "Accuracy= 0.8424102215807794\n",
      "Precision= tensor(0.6955, device='cuda:0')\n",
      "Recall= tensor(0.7283, device='cuda:0')\n",
      "F1-Score= tensor(0.7115, device='cuda:0')\n",
      "At iteration 2600 the loss is 0.106.\n",
      "Accuracy= 0.8222896680533153\n",
      "Precision= tensor(0.6347, device='cuda:0')\n",
      "Recall= tensor(0.7868, device='cuda:0')\n",
      "F1-Score= tensor(0.7026, device='cuda:0')\n",
      "At iteration 2700 the loss is 0.169.\n",
      "Accuracy= 0.8302911961966211\n",
      "Precision= tensor(0.6538, device='cuda:0')\n",
      "Recall= tensor(0.7737, device='cuda:0')\n",
      "F1-Score= tensor(0.7087, device='cuda:0')\n",
      "At iteration 2800 the loss is 0.080.\n",
      "Accuracy= 0.8345572629255454\n",
      "Precision= tensor(0.6669, device='cuda:0')\n",
      "Recall= tensor(0.7592, device='cuda:0')\n",
      "F1-Score= tensor(0.7101, device='cuda:0')\n",
      "At iteration 2900 the loss is 0.125.\n",
      "Accuracy= 0.8336446217845318\n",
      "Precision= tensor(0.6648, device='cuda:0')\n",
      "Recall= tensor(0.7595, device='cuda:0')\n",
      "F1-Score= tensor(0.7090, device='cuda:0')\n",
      "At iteration 100 the loss is 0.035.\n",
      "Accuracy= 0.8335597249342049\n",
      "Precision= tensor(0.6650, device='cuda:0')\n",
      "Recall= tensor(0.7580, device='cuda:0')\n",
      "F1-Score= tensor(0.7085, device='cuda:0')\n",
      "At iteration 200 the loss is 0.097.\n",
      "Accuracy= 0.8334323796587146\n",
      "Precision= tensor(0.6713, device='cuda:0')\n",
      "Recall= tensor(0.7362, device='cuda:0')\n",
      "F1-Score= tensor(0.7023, device='cuda:0')\n",
      "At iteration 300 the loss is 0.023.\n",
      "Accuracy= 0.8370617200101876\n",
      "Precision= tensor(0.6817, device='cuda:0')\n",
      "Recall= tensor(0.7304, device='cuda:0')\n",
      "F1-Score= tensor(0.7052, device='cuda:0')\n",
      "At iteration 400 the loss is 0.156.\n",
      "Accuracy= 0.8247092282876305\n",
      "Precision= tensor(0.6411, device='cuda:0')\n",
      "Recall= tensor(0.7794, device='cuda:0')\n",
      "F1-Score= tensor(0.7035, device='cuda:0')\n",
      "At iteration 500 the loss is 0.038.\n",
      "Accuracy= 0.832371169029629\n",
      "Precision= tensor(0.6662, device='cuda:0')\n",
      "Recall= tensor(0.7452, device='cuda:0')\n",
      "F1-Score= tensor(0.7035, device='cuda:0')\n",
      "At iteration 600 the loss is 0.056.\n",
      "Accuracy= 0.8216317174632821\n",
      "Precision= tensor(0.6354, device='cuda:0')\n",
      "Recall= tensor(0.7778, device='cuda:0')\n",
      "F1-Score= tensor(0.6994, device='cuda:0')\n",
      "At iteration 700 the loss is 0.027.\n",
      "Accuracy= 0.8246880040750488\n",
      "Precision= tensor(0.6436, device='cuda:0')\n",
      "Recall= tensor(0.7687, device='cuda:0')\n",
      "F1-Score= tensor(0.7006, device='cuda:0')\n",
      "At iteration 800 the loss is 0.155.\n",
      "Accuracy= 0.8277230664742338\n",
      "Precision= tensor(0.6501, device='cuda:0')\n",
      "Recall= tensor(0.7673, device='cuda:0')\n",
      "F1-Score= tensor(0.7039, device='cuda:0')\n",
      "At iteration 900 the loss is 0.083.\n",
      "Accuracy= 0.8197003141183462\n",
      "Precision= tensor(0.6343, device='cuda:0')\n",
      "Recall= tensor(0.7660, device='cuda:0')\n",
      "F1-Score= tensor(0.6939, device='cuda:0')\n",
      "At iteration 1000 the loss is 0.068.\n",
      "Accuracy= 0.8301214024959674\n",
      "Precision= tensor(0.6623, device='cuda:0')\n",
      "Recall= tensor(0.7413, device='cuda:0')\n",
      "F1-Score= tensor(0.6996, device='cuda:0')\n",
      "At iteration 1100 the loss is 0.165.\n",
      "Accuracy= 0.8243908650989048\n",
      "Precision= tensor(0.6454, device='cuda:0')\n",
      "Recall= tensor(0.7589, device='cuda:0')\n",
      "F1-Score= tensor(0.6975, device='cuda:0')\n",
      "At iteration 1200 the loss is 0.103.\n",
      "Accuracy= 0.8143730367603362\n",
      "Precision= tensor(0.6224, device='cuda:0')\n",
      "Recall= tensor(0.7739, device='cuda:0')\n",
      "F1-Score= tensor(0.6899, device='cuda:0')\n",
      "At iteration 1300 the loss is 0.189.\n",
      "Accuracy= 0.8270226674590373\n",
      "Precision= tensor(0.6541, device='cuda:0')\n",
      "Recall= tensor(0.7464, device='cuda:0')\n",
      "F1-Score= tensor(0.6972, device='cuda:0')\n",
      "At iteration 1400 the loss is 0.080.\n",
      "Accuracy= 0.8323287206044656\n",
      "Precision= tensor(0.6687, device='cuda:0')\n",
      "Recall= tensor(0.7365, device='cuda:0')\n",
      "F1-Score= tensor(0.7010, device='cuda:0')\n",
      "At iteration 1500 the loss is 0.065.\n",
      "Accuracy= 0.8247304525002123\n",
      "Precision= tensor(0.6448, device='cuda:0')\n",
      "Recall= tensor(0.7638, device='cuda:0')\n",
      "F1-Score= tensor(0.6993, device='cuda:0')\n",
      "At iteration 1600 the loss is 0.086.\n",
      "Accuracy= 0.8175142202224297\n",
      "Precision= tensor(0.6287, device='cuda:0')\n",
      "Recall= tensor(0.7720, device='cuda:0')\n",
      "F1-Score= tensor(0.6930, device='cuda:0')\n",
      "At iteration 1700 the loss is 0.086.\n",
      "Accuracy= 0.8195305204176925\n",
      "Precision= tensor(0.6341, device='cuda:0')\n",
      "Recall= tensor(0.7652, device='cuda:0')\n",
      "F1-Score= tensor(0.6935, device='cuda:0')\n",
      "At iteration 1800 the loss is 0.072.\n",
      "Accuracy= 0.8303973172595297\n",
      "Precision= tensor(0.6660, device='cuda:0')\n",
      "Recall= tensor(0.7308, device='cuda:0')\n",
      "F1-Score= tensor(0.6969, device='cuda:0')\n",
      "At iteration 1900 the loss is 0.033.\n",
      "Accuracy= 0.8245818830121403\n",
      "Precision= tensor(0.6462, device='cuda:0')\n",
      "Recall= tensor(0.7570, device='cuda:0')\n",
      "F1-Score= tensor(0.6972, device='cuda:0')\n",
      "At iteration 2000 the loss is 0.126.\n",
      "Accuracy= 0.8170260633330504\n",
      "Precision= tensor(0.6282, device='cuda:0')\n",
      "Recall= tensor(0.7701, device='cuda:0')\n",
      "F1-Score= tensor(0.6919, device='cuda:0')\n",
      "At iteration 2100 the loss is 0.073.\n",
      "Accuracy= 0.8267255284828933\n",
      "Precision= tensor(0.6523, device='cuda:0')\n",
      "Recall= tensor(0.7510, device='cuda:0')\n",
      "F1-Score= tensor(0.6982, device='cuda:0')\n",
      "At iteration 2200 the loss is 0.147.\n",
      "Accuracy= 0.8282961202139401\n",
      "Precision= tensor(0.6585, device='cuda:0')\n",
      "Recall= tensor(0.7406, device='cuda:0')\n",
      "F1-Score= tensor(0.6971, device='cuda:0')\n",
      "At iteration 2300 the loss is 0.135.\n",
      "Accuracy= 0.825558196790899\n",
      "Precision= tensor(0.6514, device='cuda:0')\n",
      "Recall= tensor(0.7448, device='cuda:0')\n",
      "F1-Score= tensor(0.6950, device='cuda:0')\n",
      "At iteration 2400 the loss is 0.093.\n",
      "Accuracy= 0.8212284574242296\n",
      "Precision= tensor(0.6380, device='cuda:0')\n",
      "Recall= tensor(0.7629, device='cuda:0')\n",
      "F1-Score= tensor(0.6949, device='cuda:0')\n",
      "At iteration 2500 the loss is 0.079.\n",
      "Accuracy= 0.8260251294676968\n",
      "Precision= tensor(0.6501, device='cuda:0')\n",
      "Recall= tensor(0.7537, device='cuda:0')\n",
      "F1-Score= tensor(0.6981, device='cuda:0')\n",
      "At iteration 2600 the loss is 0.087.\n",
      "Accuracy= 0.8239663808472706\n",
      "Precision= tensor(0.6413, device='cuda:0')\n",
      "Recall= tensor(0.7724, device='cuda:0')\n",
      "F1-Score= tensor(0.7007, device='cuda:0')\n",
      "At iteration 2700 the loss is 0.098.\n",
      "Accuracy= 0.8316919942270142\n",
      "Precision= tensor(0.6673, device='cuda:0')\n",
      "Recall= tensor(0.7364, device='cuda:0')\n",
      "F1-Score= tensor(0.7001, device='cuda:0')\n",
      "At iteration 2800 the loss is 0.063.\n",
      "Accuracy= 0.8291663129297903\n",
      "Precision= tensor(0.6558, device='cuda:0')\n",
      "Recall= tensor(0.7571, device='cuda:0')\n",
      "F1-Score= tensor(0.7028, device='cuda:0')\n",
      "At iteration 2900 the loss is 0.100.\n",
      "Accuracy= 0.8255369725783174\n",
      "Precision= tensor(0.6464, device='cuda:0')\n",
      "Recall= tensor(0.7642, device='cuda:0')\n",
      "F1-Score= tensor(0.7004, device='cuda:0')\n",
      "At iteration 100 the loss is 0.022.\n",
      "Accuracy= 0.8261736989557688\n",
      "Precision= tensor(0.6488, device='cuda:0')\n",
      "Recall= tensor(0.7599, device='cuda:0')\n",
      "F1-Score= tensor(0.7000, device='cuda:0')\n",
      "At iteration 200 the loss is 0.133.\n",
      "Accuracy= 0.8247516767127939\n",
      "Precision= tensor(0.6471, device='cuda:0')\n",
      "Recall= tensor(0.7550, device='cuda:0')\n",
      "F1-Score= tensor(0.6969, device='cuda:0')\n",
      "At iteration 300 the loss is 0.059.\n",
      "Accuracy= 0.8306307835979285\n",
      "Precision= tensor(0.6604, device='cuda:0')\n",
      "Recall= tensor(0.7518, device='cuda:0')\n",
      "F1-Score= tensor(0.7032, device='cuda:0')\n",
      "At iteration 400 the loss is 0.018.\n",
      "Accuracy= 0.8318617879276679\n",
      "Precision= tensor(0.6657, device='cuda:0')\n",
      "Recall= tensor(0.7429, device='cuda:0')\n",
      "F1-Score= tensor(0.7022, device='cuda:0')\n",
      "At iteration 500 the loss is 0.021.\n",
      "Accuracy= 0.8317344426521776\n",
      "Precision= tensor(0.6704, device='cuda:0')\n",
      "Recall= tensor(0.7268, device='cuda:0')\n",
      "F1-Score= tensor(0.6974, device='cuda:0')\n",
      "At iteration 600 the loss is 0.107.\n",
      "Accuracy= 0.8331989133203158\n",
      "Precision= tensor(0.6723, device='cuda:0')\n",
      "Recall= tensor(0.7313, device='cuda:0')\n",
      "F1-Score= tensor(0.7006, device='cuda:0')\n",
      "At iteration 700 the loss is 0.031.\n",
      "Accuracy= 0.8336870702096952\n",
      "Precision= tensor(0.6704, device='cuda:0')\n",
      "Recall= tensor(0.7411, device='cuda:0')\n",
      "F1-Score= tensor(0.7040, device='cuda:0')\n",
      "At iteration 800 the loss is 0.089.\n",
      "Accuracy= 0.8280838780881229\n",
      "Precision= tensor(0.6522, device='cuda:0')\n",
      "Recall= tensor(0.7621, device='cuda:0')\n",
      "F1-Score= tensor(0.7029, device='cuda:0')\n",
      "At iteration 900 the loss is 0.085.\n",
      "Accuracy= 0.8343874692248917\n",
      "Precision= tensor(0.6706, device='cuda:0')\n",
      "Recall= tensor(0.7455, device='cuda:0')\n",
      "F1-Score= tensor(0.7061, device='cuda:0')\n",
      "At iteration 1000 the loss is 0.037.\n",
      "Accuracy= 0.8229688428559301\n",
      "Precision= tensor(0.6404, device='cuda:0')\n",
      "Recall= tensor(0.7677, device='cuda:0')\n",
      "F1-Score= tensor(0.6983, device='cuda:0')\n",
      "At iteration 1100 the loss is 0.057.\n",
      "Accuracy= 0.8297605908820783\n",
      "Precision= tensor(0.6605, device='cuda:0')\n",
      "Recall= tensor(0.7450, device='cuda:0')\n",
      "F1-Score= tensor(0.7002, device='cuda:0')\n",
      "At iteration 1200 the loss is 0.009.\n",
      "Accuracy= 0.8346846082010357\n",
      "Precision= tensor(0.6747, device='cuda:0')\n",
      "Recall= tensor(0.7347, device='cuda:0')\n",
      "F1-Score= tensor(0.7034, device='cuda:0')\n",
      "At iteration 1300 the loss is 0.012.\n",
      "Accuracy= 0.8344723660752186\n",
      "Precision= tensor(0.6718, device='cuda:0')\n",
      "Recall= tensor(0.7424, device='cuda:0')\n",
      "F1-Score= tensor(0.7053, device='cuda:0')\n",
      "At iteration 1400 the loss is 0.076.\n",
      "Accuracy= 0.8357670430427031\n",
      "Precision= tensor(0.6856, device='cuda:0')\n",
      "Recall= tensor(0.7103, device='cuda:0')\n",
      "F1-Score= tensor(0.6977, device='cuda:0')\n",
      "At iteration 1500 the loss is 0.038.\n",
      "Accuracy= 0.823053739706257\n",
      "Precision= tensor(0.6427, device='cuda:0')\n",
      "Recall= tensor(0.7587, device='cuda:0')\n",
      "F1-Score= tensor(0.6959, device='cuda:0')\n",
      "At iteration 1600 the loss is 0.063.\n",
      "Accuracy= 0.8293573308430258\n",
      "Precision= tensor(0.6586, device='cuda:0')\n",
      "Recall= tensor(0.7486, device='cuda:0')\n",
      "F1-Score= tensor(0.7007, device='cuda:0')\n",
      "At iteration 1700 the loss is 0.044.\n",
      "Accuracy= 0.8262798200186773\n",
      "Precision= tensor(0.6499, device='cuda:0')\n",
      "Recall= tensor(0.7566, device='cuda:0')\n",
      "F1-Score= tensor(0.6992, device='cuda:0')\n",
      "At iteration 1800 the loss is 0.081.\n",
      "Accuracy= 0.8169411664827235\n",
      "Precision= tensor(0.6272, device='cuda:0')\n",
      "Recall= tensor(0.7739, device='cuda:0')\n",
      "F1-Score= tensor(0.6929, device='cuda:0')\n",
      "At iteration 1900 the loss is 0.106.\n",
      "Accuracy= 0.8272136853722727\n",
      "Precision= tensor(0.6547, device='cuda:0')\n",
      "Recall= tensor(0.7458, device='cuda:0')\n",
      "F1-Score= tensor(0.6973, device='cuda:0')\n",
      "At iteration 2000 the loss is 0.053.\n",
      "Accuracy= 0.8149248662874607\n",
      "Precision= tensor(0.6211, device='cuda:0')\n",
      "Recall= tensor(0.7856, device='cuda:0')\n",
      "F1-Score= tensor(0.6938, device='cuda:0')\n",
      "At iteration 2100 the loss is 0.135.\n",
      "Accuracy= 0.8286357076152475\n",
      "Precision= tensor(0.6553, device='cuda:0')\n",
      "Recall= tensor(0.7547, device='cuda:0')\n",
      "F1-Score= tensor(0.7015, device='cuda:0')\n",
      "At iteration 2200 the loss is 0.201.\n",
      "Accuracy= 0.8296120213940062\n",
      "Precision= tensor(0.6573, device='cuda:0')\n",
      "Recall= tensor(0.7552, device='cuda:0')\n",
      "F1-Score= tensor(0.7028, device='cuda:0')\n",
      "At iteration 2300 the loss is 0.125.\n",
      "Accuracy= 0.8220562017149163\n",
      "Precision= tensor(0.6373, device='cuda:0')\n",
      "Recall= tensor(0.7731, device='cuda:0')\n",
      "F1-Score= tensor(0.6987, device='cuda:0')\n",
      "At iteration 2400 the loss is 0.112.\n",
      "Accuracy= 0.828444689702012\n",
      "Precision= tensor(0.6550, device='cuda:0')\n",
      "Recall= tensor(0.7543, device='cuda:0')\n",
      "F1-Score= tensor(0.7012, device='cuda:0')\n",
      "At iteration 2500 the loss is 0.029.\n",
      "Accuracy= 0.8248577977757026\n",
      "Precision= tensor(0.6451, device='cuda:0')\n",
      "Recall= tensor(0.7638, device='cuda:0')\n",
      "F1-Score= tensor(0.6995, device='cuda:0')\n",
      "At iteration 2600 the loss is 0.141.\n",
      "Accuracy= 0.8219076322268444\n",
      "Precision= tensor(0.6356, device='cuda:0')\n",
      "Recall= tensor(0.7793, device='cuda:0')\n",
      "F1-Score= tensor(0.7002, device='cuda:0')\n",
      "At iteration 2700 the loss is 0.043.\n",
      "Accuracy= 0.8373164105611682\n",
      "Precision= tensor(0.6880, device='cuda:0')\n",
      "Recall= tensor(0.7141, device='cuda:0')\n",
      "F1-Score= tensor(0.7008, device='cuda:0')\n",
      "At iteration 2800 the loss is 0.026.\n",
      "Accuracy= 0.8336021733593684\n",
      "Precision= tensor(0.6733, device='cuda:0')\n",
      "Recall= tensor(0.7313, device='cuda:0')\n",
      "F1-Score= tensor(0.7011, device='cuda:0')\n",
      "At iteration 2900 the loss is 0.127.\n",
      "Accuracy= 0.8233933271075643\n",
      "Precision= tensor(0.6452, device='cuda:0')\n",
      "Recall= tensor(0.7512, device='cuda:0')\n",
      "F1-Score= tensor(0.6942, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "GRAD_CLIP = 1.\n",
    "NUM_EPOCHS = 10\n",
    "VOCAB_SIZE = len(vocab_words)\n",
    "EMBEDDING_SIZE = 300\n",
    "OUTPUT_DIM = 2\n",
    "\n",
    "# def repackage_hidden(h):\n",
    "#     \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "#     if h is None:\n",
    "#         return None\n",
    "#     elif isinstance(h, torch.Tensor):\n",
    "#         return h.detach()\n",
    "#     else:\n",
    "#         return tuple(repackage_hidden(v) for v in h)\n",
    "\n",
    "model = RNNLM(\"LSTM\", VOCAB_SIZE, EMBEDDING_SIZE, EMBEDDING_SIZE, 2, OUTPUT_DIM, dropout=0.5)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "\n",
    "# Using weighted loss given the class is imbalanced and also loss should be the size of the batch\n",
    "# loss_fn = nn.CrossEntropyLoss(weight=class_weights.cuda(), reduction='none') ## Used instead of NLLLoss.\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights.cuda())\n",
    "# loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "# loss_fn = F.binary_cross_entropy_with_logits()\n",
    "\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "val_losses = []\n",
    "best_f1score = 0\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "f1score_list = []\n",
    "least_loss = math.inf\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # print(\"Epoch 1\")\n",
    "    model.train()\n",
    "    it = iter(train_dataloader)\n",
    "    # print(\"Length of the total batches=\", it)\n",
    "    log_interval_1 = 100\n",
    "    log_interval_2 = 100\n",
    "    # There are no hidden tensors for the first batch, and so will default to zeros.\n",
    "    # hidden = None\n",
    "    for i, batch in enumerate(it):\n",
    "      # print(\"Length of the batch=\", batch[0].shape)\n",
    "      text, target, sample_weight = batch\n",
    "      if USE_CUDA:\n",
    "        text, target, sample_weight = text.cuda(), target.cuda(), sample_weight.cuda()\n",
    "        # print(\"Target:\", target)\n",
    "      # hidden = None\n",
    "      # hidden = repackage_hidden(hidden)\n",
    "      # print(text)\n",
    "      model.zero_grad()\n",
    "\n",
    "      # forward pass\n",
    "      output = model(text)\n",
    "      # pred = torch.argmax(F.softmax(output, dim=1), dim=1)\n",
    "      # print(\"Output: \", output.shape, output)\n",
    "      # print(\"Prediction: \", pred.shape, pred)\n",
    "      # print(\"Shape=\", output.shape)\n",
    "      # print(output.size(-1))\n",
    "      # print(output.view(-1, output.size(-1)).shape)\n",
    "      # print(target.view(-1).shape)\n",
    "      loss = loss_fn(output.view(-1, output.size(-1)), target.view(-1))\n",
    "      # loss = F.binary_cross_entropy_with_logits(pred, target.view(-1), weight=sample_weight, pos_weight=class_weight)\n",
    "      # loss = loss * sample_weight\n",
    "      # loss = loss.mean()\n",
    "\n",
    "      #back propagation\n",
    "      loss.backward()\n",
    "      nn.utils.clip_grad_norm_(model.parameters(), max_norm=GRAD_CLIP, norm_type=2)\n",
    "      optimizer.step()\n",
    "\n",
    "      if i % log_interval_1 == 0 and i > 0:\n",
    "        print(f'At iteration {i} the loss is {loss:.3f}.')\n",
    "\n",
    "      if i % log_interval_2 == 0 and i > 0:\n",
    "        val_loss, precision, recall, f1score = evaluate(model, valid_dataloader)\n",
    "        val_losses.append(val_loss)\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        f1score_list.append(f1score)\n",
    "        if f1score > best_f1score:\n",
    "          best_f1score = f1score\n",
    "          best_model = type(model)(\"LSTM\", VOCAB_SIZE, EMBEDDING_SIZE, EMBEDDING_SIZE, 2, OUTPUT_DIM, dropout=0.5) # get a new instance\n",
    "          best_model.load_state_dict(model.state_dict()) # copy weights and biases  \n",
    "        model.train()   \n",
    "        \n",
    "        ''' Do the following:\n",
    "            - Extract the text and target from the batch, and if using CUDA (essentially, using GPUs), place \n",
    "              the tensors on cuda, using a commands such as \"text = text.cuda()\".  More details are at\n",
    "              https://pytorch.org/docs/stable/tensors.html#torch.Tensor.cuda\n",
    "            - Pass the hidden state vector from output of previous batch as the initial hidden vector for\n",
    "              the current batch. But detach each tensor in the hidden state vector using tensor.detach() or\n",
    "              the provided repackage_hidden(). See\n",
    "              https://pytorch.org/docs/master/generated/torch.Tensor.detach_.html#torch-tensor-detach\n",
    "            - Zero out the model gradients to reset backpropagation for current batch\n",
    "            - Call forward propagation to get output and final hidden state vector.\n",
    "            - Compute the cross entropy loss\n",
    "            - Run back propagation to set the gradients for each model parameter.\n",
    "            - Clip the gradients that may have exploded. See Sec 5.2.4 in the Goldberg textbook, and\n",
    "              https://pytorch.org/docs/master/generated/torch.nn.utils.clip_grad_norm_.html#torch-nn-utils-clip-grad-norm\n",
    "            - Run a step of gradient descent. \n",
    "            - Print the batch loss after every few iterations. (Say every 100 when developing, every 1000 otherwise.)\n",
    "            - Evaluate your model on the validation set after every, say, 10000 iterations and save it to val_losses. If\n",
    "              your model has the lowest validation loss so far, copy it to best_model. For that it is recommended that\n",
    "              copy the state_dict rather than use deepcopy, since the latter doesn't work on Colab.  See discussion at \n",
    "              https://discuss.pytorch.org/t/deep-copying-pytorch-modules/13514. This is Early Stopping and is described\n",
    "              in Sec 2.3.1 of Lecture notes by Cho: \n",
    "              https://github.com/nyu-dl/NLP_DL_Lecture_Note/blob/master/lecture_note.pdf\n",
    "        '''\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 0.8648581997533908\n",
      "Precision= tensor(0.3522, device='cuda:0')\n",
      "Recall= tensor(0.8513, device='cuda:0')\n",
      "F1-Score= tensor(0.4982, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "torch.save(best_model.state_dict(), \"best-model-parameters-lstm-class-weights-without-embed-f1score_keepstop_undersampling1pt\")\n",
    "accuracy, precision, recall, f1score = evaluate(best_model.cuda(), test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 0.8447184545828196\n",
      "Precision= tensor(0.3087, device='cuda:0')\n",
      "Recall= tensor(0.7824, device='cuda:0')\n",
      "F1-Score= tensor(0.4427, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall, f1score = evaluate(model.cuda(), test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, score in enumerate(f1score_list):\n",
    "    # print(i, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(58)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(torch.tensor(f1score_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************\n",
      "Training Accuracy= 0.6686202791905638\n",
      "Training Precision= tensor(0.6617, device='cuda:0')\n",
      "Training Recall= tensor(0.6923, device='cuda:0')\n",
      "Training F1-Score= tensor(0.6766, device='cuda:0')\n",
      "Accuracy= 0.8602939903388334\n",
      "Precision= tensor(0.8779, device='cuda:0')\n",
      "Recall= tensor(0.8342, device='cuda:0')\n",
      "F1-Score= tensor(0.8555, device='cuda:0')\n",
      "******************\n",
      "Training Accuracy= 0.7730418785845687\n",
      "Training Precision= tensor(0.7651, device='cuda:0')\n",
      "Training Recall= tensor(0.7891, device='cuda:0')\n",
      "Training F1-Score= tensor(0.7769, device='cuda:0')\n",
      "Accuracy= 0.8795816957251935\n",
      "Precision= tensor(0.8934, device='cuda:0')\n",
      "Recall= tensor(0.8598, device='cuda:0')\n",
      "F1-Score= tensor(0.8762, device='cuda:0')\n",
      "******************\n",
      "Training Accuracy= 0.8155812862965769\n",
      "Training Precision= tensor(0.8058, device='cuda:0')\n",
      "Training Recall= tensor(0.8323, device='cuda:0')\n",
      "Training F1-Score= tensor(0.8189, device='cuda:0')\n",
      "Accuracy= 0.8850355801028447\n",
      "Precision= tensor(0.8760, device='cuda:0')\n",
      "Recall= tensor(0.8948, device='cuda:0')\n",
      "F1-Score= tensor(0.8853, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "GRAD_CLIP = 1.\n",
    "NUM_EPOCHS = 3\n",
    "VOCAB_SIZE = len(vocab_words)\n",
    "EMBEDDING_SIZE = 300\n",
    "OUTPUT_DIM = 2\n",
    "\n",
    "# def repackage_hidden(h):\n",
    "#     \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "#     if h is None:\n",
    "#         return None\n",
    "#     elif isinstance(h, torch.Tensor):\n",
    "#         return h.detach()\n",
    "#     else:\n",
    "#         return tuple(repackage_hidden(v) for v in h)\n",
    "\n",
    "model = RNNLM(\"LSTM\", VOCAB_SIZE, EMBEDDING_SIZE, EMBEDDING_SIZE, 2, OUTPUT_DIM, dropout=0.5)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "\n",
    "# Using weighted loss given the class is imbalanced and also loss should be the size of the batch\n",
    "# loss_fn = nn.CrossEntropyLoss(weight=class_weights.cuda(), reduction='none') ## Used instead of NLLLoss.\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "# loss_fn = F.binary_cross_entropy_with_logits()\n",
    "\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "val_losses = []\n",
    "best_f1score = 0\n",
    "train_total_correct_pred = 0\n",
    "val_precision_list = []\n",
    "val_recall_list = []\n",
    "val_f1score_list = []\n",
    "val_accuracy_list = []\n",
    "train_precision_list = []\n",
    "train_recall_list = []\n",
    "train_f1score_list = []\n",
    "train_accuracy_list = []\n",
    "train_total_pred = 0\n",
    "train_true_positives = 0\n",
    "train_predicted_positives = 0\n",
    "least_loss = math.inf\n",
    "train_actual_positives = 0\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(\"******************\")\n",
    "    model.train()\n",
    "    it = iter(train_dataloader)\n",
    "    # print(\"Length of the total batches=\", it)\n",
    "    log_interval_1 = 100\n",
    "    log_interval_2 = 100\n",
    "    # There are no hidden tensors for the first batch, and so will default to zeros.\n",
    "    # hidden = None\n",
    "    for i, batch in enumerate(it):\n",
    "      # print(\"Length of the batch=\", batch[0].shape)\n",
    "      text, target, sample_weight = batch\n",
    "      if USE_CUDA:\n",
    "        text, target, sample_weight = text.cuda(), target.cuda(), sample_weight.cuda()\n",
    "        # print(\"Target:\", target)\n",
    "      # hidden = None\n",
    "      # hidden = repackage_hidden(hidden)\n",
    "      # print(text)\n",
    "      model.zero_grad()\n",
    "\n",
    "      # forward pass\n",
    "      output = model(text)\n",
    "      # pred = torch.argmax(F.softmax(output, dim=1), dim=1)\n",
    "      # print(\"Output: \", output.shape, output)\n",
    "      # print(\"Prediction: \", pred.shape, pred)\n",
    "      # print(\"Shape=\", output.shape)\n",
    "      # print(output.size(-1))\n",
    "      # print(output.view(-1, output.size(-1)).shape)\n",
    "      # print(target.view(-1).shape)\n",
    "      loss = loss_fn(output.view(-1, output.size(-1)), target.view(-1))\n",
    "      pred = F.softmax(output.view(-1, output.size(-1)), dim=1)\n",
    "      pred = pred.argmax(dim=1)\n",
    "      # loss = F.binary_cross_entropy_with_logits(pred, target.view(-1), weight=sample_weight, pos_weight=class_weight)\n",
    "      # loss = loss * sample_weight\n",
    "      # loss = loss.mean()\n",
    "      # calculate accuracy      \n",
    "      train_total_pred += len(pred)\n",
    "      train_total_correct_pred += int((pred==target.view(-1)).sum())\n",
    "\n",
    "      # calculate precision & recall \n",
    "      train_true_positives += precision_score(target.view(-1).cpu(), pred.cpu()) * (torch.count_nonzero(pred))\n",
    "      train_predicted_positives += ((torch.count_nonzero(pred)))\n",
    "      train_actual_positives += ((torch.count_nonzero(target.view(-1))))\n",
    "\n",
    "      #back propagation\n",
    "      loss.backward()\n",
    "      nn.utils.clip_grad_norm_(model.parameters(), max_norm=GRAD_CLIP, norm_type=2)\n",
    "      optimizer.step()\n",
    "\n",
    "    accuracy = (train_total_correct_pred / train_total_pred)\n",
    "    precision = (train_true_positives/train_predicted_positives)\n",
    "    recall = (train_true_positives/train_actual_positives)\n",
    "    f1score = (2*precision*recall)/(precision+recall)\n",
    "    train_precision_list.append(precision)\n",
    "    train_recall_list.append(recall)\n",
    "    train_f1score_list.append(f1score)\n",
    "    print(\"Training Accuracy=\", accuracy)\n",
    "    print(\"Training Precision=\", precision)\n",
    "    print(\"Training Recall=\", recall)\n",
    "    print(\"Training F1-Score=\", f1score)\n",
    "\n",
    "    val_loss, precision, recall, f1score = evaluate(model, valid_dataloader)\n",
    "    val_losses.append(val_loss)\n",
    "    val_precision_list.append(precision)\n",
    "    val_recall_list.append(recall)\n",
    "    val_f1score_list.append(f1score)\n",
    "    if f1score > best_f1score:\n",
    "      best_f1score = f1score\n",
    "      best_model = type(model)(\"LSTM\", VOCAB_SIZE, EMBEDDING_SIZE, EMBEDDING_SIZE, 2, OUTPUT_DIM, dropout=0.5) # get a new instance\n",
    "      best_model.load_state_dict(model.state_dict()) # copy weights and biases  \n",
    "    model.train()   \n",
    "\n",
    "            # calculate recall\n",
    "            # true_positives += (recall_score(target.view(-1).cpu(), pred.cpu()) * (torch.count_nonzero(target.view(-1))))\n",
    "            \n",
    "            # recall = true_positives/actual_positives\n",
    "                \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 0.8757809288943691\n",
      "Precision= tensor(0.3782, device='cuda:0')\n",
      "Recall= tensor(0.8938, device='cuda:0')\n",
      "F1-Score= tensor(0.5315, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "torch.save(best_model.state_dict(), \"best-model-parameters-lstm-class-weights-without-embed-f1score_keepstop_undersampling1byclassweights_droput.pt\")\n",
    "accuracy, precision, recall, f1score = evaluate(best_model.cuda(), test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.argmax(F.softmax(torch.tensor([[0.5105, 0.4895],\n",
    "#         [0.5218, 0.4782],\n",
    "#         [0.5047, 0.4953],\n",
    "#         [0.5155, 0.4845],\n",
    "#         [0.5158, 0.4842],\n",
    "#         [0.5105, 0.4895],\n",
    "#         [0.4782, 0.5218],\n",
    "#         [0.5047, 0.4953],\n",
    "#         [0.5155, 0.4845],\n",
    "#         [0.5158, 0.4842],])), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model.state_dict(), \"best-model-parameters-lstm-class-weights-without-embed-f1score_keepstop.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 0.8996917385943279\n",
      "Precision= tensor(0.4315, device='cuda:0')\n",
      "Recall= tensor(0.8584, device='cuda:0')\n",
      "F1-Score= tensor(0.5743, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall, f1score = evaluate(best_model.cuda(), test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train an LSTM Model: with UnFrozen embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 1000 the loss is 0.685.\n",
      "At iteration 2000 the loss is 0.681.\n",
      "At iteration 3000 the loss is 0.708.\n",
      "At iteration 4000 the loss is 0.688.\n",
      "At iteration 5000 the loss is 0.707.\n",
      "At iteration 6000 the loss is 0.622.\n",
      "At iteration 7000 the loss is 0.710.\n",
      "At iteration 8000 the loss is 0.694.\n",
      "At iteration 9000 the loss is 0.708.\n",
      "At iteration 10000 the loss is 0.701.\n",
      "Accuracy= 0.9175760094189348\n",
      "Precision= tensor(0.1180, device='cuda:0')\n",
      "Recall= tensor(0.0043, device='cuda:0')\n",
      "F1-Score= tensor(0.0083, device='cuda:0')\n",
      "At iteration 11000 the loss is 0.363.\n",
      "At iteration 12000 the loss is 0.312.\n",
      "At iteration 13000 the loss is 0.381.\n",
      "At iteration 14000 the loss is 0.316.\n",
      "At iteration 15000 the loss is 0.496.\n",
      "At iteration 16000 the loss is 0.687.\n",
      "At iteration 17000 the loss is 0.834.\n",
      "At iteration 18000 the loss is 0.291.\n",
      "At iteration 19000 the loss is 0.428.\n",
      "At iteration 20000 the loss is 0.345.\n",
      "Accuracy= 0.8853549414779417\n",
      "Precision= tensor(0.4019, device='cuda:0')\n",
      "Recall= tensor(0.8793, device='cuda:0')\n",
      "F1-Score= tensor(0.5516, device='cuda:0')\n",
      "At iteration 21000 the loss is 0.297.\n",
      "At iteration 22000 the loss is 0.675.\n",
      "At iteration 1000 the loss is 0.313.\n",
      "At iteration 2000 the loss is 0.560.\n",
      "At iteration 3000 the loss is 0.200.\n",
      "At iteration 4000 the loss is 0.170.\n",
      "At iteration 5000 the loss is 0.196.\n",
      "At iteration 6000 the loss is 0.182.\n",
      "At iteration 7000 the loss is 0.373.\n",
      "At iteration 8000 the loss is 0.406.\n",
      "At iteration 9000 the loss is 0.226.\n",
      "At iteration 10000 the loss is 0.420.\n",
      "Accuracy= 0.8760551284715008\n",
      "Precision= tensor(0.3836, device='cuda:0')\n",
      "Recall= tensor(0.8985, device='cuda:0')\n",
      "F1-Score= tensor(0.5376, device='cuda:0')\n",
      "At iteration 11000 the loss is 0.190.\n",
      "At iteration 12000 the loss is 0.151.\n",
      "At iteration 13000 the loss is 0.310.\n",
      "At iteration 14000 the loss is 0.169.\n",
      "At iteration 15000 the loss is 0.222.\n",
      "At iteration 16000 the loss is 0.124.\n",
      "At iteration 17000 the loss is 0.240.\n",
      "At iteration 18000 the loss is 0.107.\n",
      "At iteration 19000 the loss is 0.270.\n",
      "At iteration 20000 the loss is 0.236.\n",
      "Accuracy= 0.8819752060391994\n",
      "Precision= tensor(0.3956, device='cuda:0')\n",
      "Recall= tensor(0.8932, device='cuda:0')\n",
      "F1-Score= tensor(0.5483, device='cuda:0')\n",
      "At iteration 21000 the loss is 0.515.\n",
      "At iteration 22000 the loss is 0.330.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "GRAD_CLIP = 1.\n",
    "NUM_EPOCHS = 2\n",
    "VOCAB_SIZE = len(vocab_words)\n",
    "EMBEDDING_SIZE = 300\n",
    "OUTPUT_DIM = 2\n",
    "\n",
    "# def repackage_hidden(h):\n",
    "#     \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "#     if h is None:\n",
    "#         return None\n",
    "#     elif isinstance(h, torch.Tensor):\n",
    "#         return h.detach()\n",
    "#     else:\n",
    "#         return tuple(repackage_hidden(v) for v in h)\n",
    "\n",
    "model = RNNLM(\"LSTM\", VOCAB_SIZE, EMBEDDING_SIZE, EMBEDDING_SIZE, 2, OUTPUT_DIM, dropout=0.5, use_fasttext=True, freeze_fasttext=False)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "\n",
    "# Using weighted loss given the class is imbalanced and also loss should be the size of the batch\n",
    "# loss_fn = nn.CrossEntropyLoss(weight=class_weights.cuda(), reduction='none') ## Used instead of NLLLoss.\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights.cuda())\n",
    "# loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "val_losses = []\n",
    "best_f1score = 0\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "f1score_list = []\n",
    "least_loss = math.inf\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # print(\"Epoch 1\")\n",
    "    model.train()\n",
    "    it = iter(train_dataloader)\n",
    "    # print(\"Length of the total batches=\", it)\n",
    "    log_interval_1 = 1000\n",
    "    log_interval_2 = 10000\n",
    "    # There are no hidden tensors for the first batch, and so will default to zeros.\n",
    "    # hidden = None\n",
    "    for i, batch in enumerate(it):\n",
    "      # print(\"Length of the batch=\", batch[0].shape)\n",
    "      text, target, sample_weight = batch\n",
    "      if USE_CUDA:\n",
    "        text, target, sample_weight = text.cuda(), target.cuda(), sample_weight.cuda()\n",
    "      # hidden = None\n",
    "      # hidden = repackage_hidden(hidden)\n",
    "      # print(text)\n",
    "      model.zero_grad()\n",
    "\n",
    "      # forward pass\n",
    "      output = model(text)\n",
    "      # print(\"Shape=\", output.shape)\n",
    "      # print(output.size(-1))\n",
    "      # print(output.view(-1, output.size(-1)).shape)\n",
    "      # print(target.view(-1).shape)\n",
    "      loss = loss_fn(output.view(-1, output.size(-1)), target.view(-1))\n",
    "      # loss = loss * sample_weight\n",
    "      # loss = loss.mean()\n",
    "\n",
    "      #back propagation\n",
    "      loss.backward()\n",
    "      nn.utils.clip_grad_norm_(model.parameters(), max_norm=GRAD_CLIP, norm_type=2)\n",
    "      optimizer.step()\n",
    "\n",
    "      if i % log_interval_1 == 0 and i > 0:\n",
    "        print(f'At iteration {i} the loss is {loss:.3f}.')\n",
    "\n",
    "\n",
    "    val_loss, precision, recall, f1score = evaluate(model, valid_dataloader)\n",
    "    val_losses.append(val_loss)\n",
    "    precision_list.append(precision)\n",
    "    f1score_list.append(f1score)\n",
    "    recall_list.append(recall)\n",
    "    if f1score > best_f1score:\n",
    "      best_f1score = f1score\n",
    "      best_model = type(model)(\"LSTM\", VOCAB_SIZE, EMBEDDING_SIZE, EMBEDDING_SIZE, 2, OUTPUT_DIM, dropout=0.5, use_fasttext=True, freeze_fasttext=False) # get a new instance\n",
    "      best_model.load_state_dict(model.state_dict()) # copy weights and biases  \n",
    "    model.train()   \n",
    "        \n",
    "        ''' Do the following:\n",
    "            - Extract the text and target from the batch, and if using CUDA (essentially, using GPUs), place \n",
    "              the tensors on cuda, using a commands such as \"text = text.cuda()\".  More details are at\n",
    "              https://pytorch.org/docs/stable/tensors.html#torch.Tensor.cuda\n",
    "            - Pass the hidden state vector from output of previous batch as the initial hidden vector for\n",
    "              the current batch. But detach each tensor in the hidden state vector using tensor.detach() or\n",
    "              the provided repackage_hidden(). See\n",
    "              https://pytorch.org/docs/master/generated/torch.Tensor.detach_.html#torch-tensor-detach\n",
    "            - Zero out the model gradients to reset backpropagation for current batch\n",
    "            - Call forward propagation to get output and final hidden state vector.\n",
    "            - Compute the cross entropy loss\n",
    "            - Run back propagation to set the gradients for each model parameter.\n",
    "            - Clip the gradients that may have exploded. See Sec 5.2.4 in the Goldberg textbook, and\n",
    "              https://pytorch.org/docs/master/generated/torch.nn.utils.clip_grad_norm_.html#torch-nn-utils-clip-grad-norm\n",
    "            - Run a step of gradient descent. \n",
    "            - Print the batch loss after every few iterations. (Say every 100 when developing, every 1000 otherwise.)\n",
    "            - Evaluate your model on the validation set after every, say, 10000 iterations and save it to val_losses. If\n",
    "              your model has the lowest validation loss so far, copy it to best_model. For that it is recommended that\n",
    "              copy the state_dict rather than use deepcopy, since the latter doesn't work on Colab.  See discussion at \n",
    "              https://discuss.pytorch.org/t/deep-copying-pytorch-modules/13514. This is Early Stopping and is described\n",
    "              in Sec 2.3.1 of Lecture notes by Cho: \n",
    "              https://github.com/nyu-dl/NLP_DL_Lecture_Note/blob/master/lecture_note.pdf\n",
    "        '''\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model.state_dict(), \"best-model-parameters-lstm-class-weights-f1score-with-unfrozenembed.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 0.8870838471023428\n",
      "Precision= tensor(0.4019, device='cuda:0')\n",
      "Recall= tensor(0.8857, device='cuda:0')\n",
      "F1-Score= tensor(0.5529, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "accuracy, precision, recall, f1score = evaluate(best_model.cuda(), test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 1000 the loss is 0.698.\n",
      "At iteration 2000 the loss is 0.722.\n",
      "At iteration 3000 the loss is 0.642.\n",
      "At iteration 4000 the loss is 0.685.\n",
      "At iteration 5000 the loss is 0.576.\n",
      "At iteration 6000 the loss is 0.326.\n",
      "At iteration 7000 the loss is 0.166.\n",
      "At iteration 8000 the loss is 0.208.\n",
      "At iteration 9000 the loss is 0.297.\n",
      "At iteration 10000 the loss is 0.351.\n",
      "Accuracy= 0.879725742779971\n",
      "Precision= tensor(0.3883, device='cuda:0')\n",
      "Recall= tensor(0.8689, device='cuda:0')\n",
      "At iteration 11000 the loss is 0.260.\n",
      "At iteration 12000 the loss is 0.313.\n",
      "At iteration 13000 the loss is 0.509.\n",
      "At iteration 14000 the loss is 0.244.\n",
      "At iteration 15000 the loss is 0.184.\n",
      "At iteration 16000 the loss is 0.295.\n",
      "At iteration 17000 the loss is 0.223.\n",
      "At iteration 18000 the loss is 0.303.\n",
      "At iteration 19000 the loss is 0.371.\n",
      "At iteration 20000 the loss is 0.282.\n",
      "Accuracy= 0.8948819170302652\n",
      "Precision= tensor(0.4243, device='cuda:0')\n",
      "Recall= tensor(0.8709, device='cuda:0')\n",
      "At iteration 21000 the loss is 0.514.\n",
      "At iteration 22000 the loss is 0.131.\n",
      "At iteration 1000 the loss is 0.201.\n",
      "At iteration 2000 the loss is 0.164.\n",
      "At iteration 3000 the loss is 0.278.\n",
      "At iteration 4000 the loss is 0.396.\n",
      "At iteration 5000 the loss is 0.291.\n",
      "At iteration 6000 the loss is 0.160.\n",
      "At iteration 7000 the loss is 0.282.\n",
      "At iteration 8000 the loss is 0.184.\n",
      "At iteration 9000 the loss is 0.172.\n",
      "At iteration 10000 the loss is 0.395.\n",
      "Accuracy= 0.8854435902763349\n",
      "Precision= tensor(0.4026, device='cuda:0')\n",
      "Recall= tensor(0.8849, device='cuda:0')\n",
      "At iteration 11000 the loss is 0.137.\n",
      "At iteration 12000 the loss is 0.357.\n",
      "At iteration 13000 the loss is 0.195.\n",
      "At iteration 14000 the loss is 0.163.\n",
      "At iteration 15000 the loss is 0.412.\n",
      "At iteration 16000 the loss is 0.160.\n",
      "At iteration 17000 the loss is 0.393.\n",
      "At iteration 18000 the loss is 0.156.\n",
      "At iteration 19000 the loss is 0.352.\n",
      "At iteration 20000 the loss is 0.254.\n",
      "Accuracy= 0.8661070711268093\n",
      "Precision= tensor(0.3650, device='cuda:0')\n",
      "Recall= tensor(0.9048, device='cuda:0')\n",
      "At iteration 21000 the loss is 0.227.\n",
      "At iteration 22000 the loss is 0.302.\n",
      "At iteration 1000 the loss is 0.202.\n",
      "At iteration 2000 the loss is 0.240.\n",
      "At iteration 3000 the loss is 0.142.\n",
      "At iteration 4000 the loss is 0.158.\n",
      "At iteration 5000 the loss is 0.225.\n",
      "At iteration 6000 the loss is 0.160.\n",
      "At iteration 7000 the loss is 0.134.\n",
      "At iteration 8000 the loss is 0.337.\n",
      "At iteration 9000 the loss is 0.117.\n",
      "At iteration 10000 the loss is 0.125.\n",
      "Accuracy= 0.8928623865918692\n",
      "Precision= tensor(0.4183, device='cuda:0')\n",
      "Recall= tensor(0.8599, device='cuda:0')\n",
      "At iteration 11000 the loss is 0.066.\n",
      "At iteration 12000 the loss is 0.319.\n",
      "At iteration 13000 the loss is 0.268.\n",
      "At iteration 14000 the loss is 0.685.\n",
      "At iteration 15000 the loss is 0.192.\n",
      "At iteration 16000 the loss is 0.210.\n",
      "At iteration 17000 the loss is 0.283.\n",
      "At iteration 18000 the loss is 0.191.\n",
      "At iteration 19000 the loss is 0.183.\n",
      "At iteration 20000 the loss is 0.148.\n",
      "Accuracy= 0.8981259089964679\n",
      "Precision= tensor(0.4321, device='cuda:0')\n",
      "Recall= tensor(0.8593, device='cuda:0')\n",
      "At iteration 21000 the loss is 0.201.\n",
      "At iteration 22000 the loss is 0.185.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "GRAD_CLIP = 1.\n",
    "NUM_EPOCHS = 3\n",
    "VOCAB_SIZE = len(vocab_words)\n",
    "EMBEDDING_SIZE = 300\n",
    "OUTPUT_DIM = 2\n",
    "\n",
    "# def repackage_hidden(h):\n",
    "#     \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "#     if h is None:\n",
    "#         return None\n",
    "#     elif isinstance(h, torch.Tensor):\n",
    "#         return h.detach()\n",
    "#     else:\n",
    "#         return tuple(repackage_hidden(v) for v in h)\n",
    "\n",
    "model = RNNLM(\"LSTM\", VOCAB_SIZE, EMBEDDING_SIZE, EMBEDDING_SIZE, 2, OUTPUT_DIM, dropout=0.5, use_fasttext=True, freeze_fasttext=False)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "\n",
    "# Using weighted loss given the class is imbalanced and also loss should be the size of the batch\n",
    "# loss_fn = nn.CrossEntropyLoss(weight=class_weights.cuda(), reduction='none') ## Used instead of NLLLoss.\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights.cuda())\n",
    "# loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "val_losses = []\n",
    "best_recall = 0\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "least_loss = math.inf\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # print(\"Epoch 1\")\n",
    "    model.train()\n",
    "    it = iter(train_dataloader)\n",
    "    # print(\"Length of the total batches=\", it)\n",
    "    log_interval_1 = 1000\n",
    "    log_interval_2 = 10000\n",
    "    # There are no hidden tensors for the first batch, and so will default to zeros.\n",
    "    # hidden = None\n",
    "    for i, batch in enumerate(it):\n",
    "      # print(\"Length of the batch=\", batch[0].shape)\n",
    "      text, target, sample_weight = batch\n",
    "      if USE_CUDA:\n",
    "        text, target, sample_weight = text.cuda(), target.cuda(), sample_weight.cuda()\n",
    "      # hidden = None\n",
    "      # hidden = repackage_hidden(hidden)\n",
    "      # print(text)\n",
    "      model.zero_grad()\n",
    "\n",
    "      # forward pass\n",
    "      output = model(text)\n",
    "      # print(\"Shape=\", output.shape)\n",
    "      # print(output.size(-1))\n",
    "      # print(output.view(-1, output.size(-1)).shape)\n",
    "      # print(target.view(-1).shape)\n",
    "      loss = loss_fn(output.view(-1, output.size(-1)), target.view(-1))\n",
    "      # loss = loss * sample_weight\n",
    "      # loss = loss.mean()\n",
    "\n",
    "      #back propagation\n",
    "      loss.backward()\n",
    "      nn.utils.clip_grad_norm_(model.parameters(), max_norm=GRAD_CLIP, norm_type=2)\n",
    "      optimizer.step()\n",
    "\n",
    "      if i % log_interval_1 == 0 and i > 0:\n",
    "        print(f'At iteration {i} the loss is {loss:.3f}.')\n",
    "\n",
    "      if i % log_interval_2 == 0 and i > 0:\n",
    "        val_loss, precision, recall = evaluate(model, valid_dataloader)\n",
    "        val_losses.append(val_loss)\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        if recall > best_recall:\n",
    "          best_recall = recall\n",
    "          best_model = type(model)(\"LSTM\", VOCAB_SIZE, EMBEDDING_SIZE, EMBEDDING_SIZE, 2, OUTPUT_DIM, dropout=0.5) # get a new instance\n",
    "          best_model.load_state_dict(model.state_dict()) # copy weights and biases  \n",
    "        model.train()   \n",
    "        \n",
    "        ''' Do the following:\n",
    "            - Extract the text and target from the batch, and if using CUDA (essentially, using GPUs), place \n",
    "              the tensors on cuda, using a commands such as \"text = text.cuda()\".  More details are at\n",
    "              https://pytorch.org/docs/stable/tensors.html#torch.Tensor.cuda\n",
    "            - Pass the hidden state vector from output of previous batch as the initial hidden vector for\n",
    "              the current batch. But detach each tensor in the hidden state vector using tensor.detach() or\n",
    "              the provided repackage_hidden(). See\n",
    "              https://pytorch.org/docs/master/generated/torch.Tensor.detach_.html#torch-tensor-detach\n",
    "            - Zero out the model gradients to reset backpropagation for current batch\n",
    "            - Call forward propagation to get output and final hidden state vector.\n",
    "            - Compute the cross entropy loss\n",
    "            - Run back propagation to set the gradients for each model parameter.\n",
    "            - Clip the gradients that may have exploded. See Sec 5.2.4 in the Goldberg textbook, and\n",
    "              https://pytorch.org/docs/master/generated/torch.nn.utils.clip_grad_norm_.html#torch-nn-utils-clip-grad-norm\n",
    "            - Run a step of gradient descent. \n",
    "            - Print the batch loss after every few iterations. (Say every 100 when developing, every 1000 otherwise.)\n",
    "            - Evaluate your model on the validation set after every, say, 10000 iterations and save it to val_losses. If\n",
    "              your model has the lowest validation loss so far, copy it to best_model. For that it is recommended that\n",
    "              copy the state_dict rather than use deepcopy, since the latter doesn't work on Colab.  See discussion at \n",
    "              https://discuss.pytorch.org/t/deep-copying-pytorch-modules/13514. This is Early Stopping and is described\n",
    "              in Sec 2.3.1 of Lecture notes by Cho: \n",
    "              https://github.com/nyu-dl/NLP_DL_Lecture_Note/blob/master/lecture_note.pdf\n",
    "        '''\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 0.8681874229346486\n",
      "Precision= tensor(0.3650, device='cuda:0')\n",
      "Recall= tensor(0.9086, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "torch.save(best_model.state_dict(), \"best-model-parameters-lstm-class-weights-with-unfrozenembed.pt\")\n",
    "accuracy, precision, recall = evaluate(best_model.cuda(), test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186685"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for each in fasttext_vectors:\n",
    "    if float(each.sum()) == 0:\n",
    "        count += 1\n",
    "\n",
    "count   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186685"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "a = torch.tensor([ 0.0,  0.0,  0.0, .0,  0.0,  0.0])\n",
    "float(a.sum()) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (each, label) in enumerate(train[:200]):\n",
    "#     print(i+1, len(each))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model.state_dict(), \"best-model-parameters-lstm1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 0.07882244143033293\n",
      "Precision= tensor(0.0788, device='cuda:0')\n",
      "Recall= tensor(1., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall = evaluate(best_model.cuda(), test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c9a33a175441410c75e373f55024e874b31e7f5cdf5048282147447bba39ea22"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
